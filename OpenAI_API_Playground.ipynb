{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjqy7heO706G"
   },
   "source": [
    "# Exploring GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VpROz_FfJZn"
   },
   "source": [
    "## Using the API\n",
    "\n",
    "Now let's take a look at how to call the OpenAI API from our code, so that we don't have to manually enter inputs into the Playground.  \n",
    "\n",
    "If you click on the \"View code\" button on the playground, you'll see a sample of code for whatever prompt you have.  For example, here's the code that we have for our few-shot learning that generates a subject to study for a topic that someone is interested in:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-002\",\n",
    "  prompt=\"how to program in Python - computer science\\nfactors leading up to WW2 - history\\nbranches of government - political science\\nShakespeare's plays - English\\ncellular respiration - biology\\nrespiratory disease - medical\\nhow to sculpt - art\",\n",
    "  temperature=0.7,\n",
    "  max_tokens=256,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0\n",
    ")\n",
    "```\n",
    "This is python code, so it'll be pretty easy for us to use this as a starting point and to modify it to create a function that we can call.\n",
    "\n",
    "\n",
    "First, you'll need install the OpenAPI via pip.  You can use Unix commands in a colab notebook by prefixing them with an exclamation point. The example below uses a `%` prefix instead—this guarantees that the installation is done using the same Python version that your notebook is using. Change line 2 from `%pip...` to `!pip...` if you're running into trouble. (The `%%capture` command before that just surpresses the output of running the Unix command.  You can remove it if you want to see the progress of the command).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "396iGnE4ra9g"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jdqGfOyrmhG"
   },
   "source": [
    "Next, you will enter your secret key for the OpenAI API. You can generate your OpenAI API key [here](https://platform.openai.com/api-keys).  \n",
    "\n",
    "We will enter it as a password, so that the raw text of it doesn't get saved in your Python notebook. That would be bad in the case that you upload or share your notebook, because then other people could use your key and have you pay for their usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PIBX87qrlDd",
    "outputId": "4d0b575f-2e1a-48f4-8734-b05e5721b66f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key:\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import openai\n",
    "import os\n",
    "\n",
    "print('Enter OpenAI API key:')\n",
    "openai.api_key = getpass()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yt7VVKVtvcCM"
   },
   "source": [
    "Now let's write a function that takes a topic as input and then outputs a subject to study if you want to learn about that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "o9-oV9mnvmtD",
    "outputId": "d2aa722a-9fc1-416e-9346-ce76c68cdc5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biology'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import time\n",
    "\n",
    "def generate_subject_few_shot(topic):\n",
    "  few_shot_prompt = \"\"\"how to program in Python - computer science\n",
    "factors leading up to WW2 - history\n",
    "branches of government - political science\n",
    "Shakespeare's plays - English\n",
    "cellular respiration - biology\n",
    "respiratory disease - medical\n",
    "how to sculpt - art\n",
    "\"\"\"\n",
    "\n",
    "  # All GPT completions are generated by the chat model now. The chat model works best\n",
    "  # when it's given a baseline instruction about how GPT should behave. We'll use this one.\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"You should recognize a pattern in the prompts and generate a completion based on that pattern.\"}\n",
    "\n",
    "  client = openai.OpenAI()\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages= [\n",
    "          system_message,\n",
    "          {\"role\" : \"user\", \"content\" : few_shot_prompt + topic + \" - \"} # simulate a user prompt\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"\\n\"]\n",
    "  )\n",
    "  # I recommend putting a short wait after each call,\n",
    "  # since the rate limit for the platform is 60 requests/min.\n",
    "  # (This increases to 3000 requests/min after you've been using the platform for 2 days).\n",
    "  time.sleep(1)\n",
    "\n",
    "  # the response from OpenAI's API is a JSON object that contains\n",
    "  # the completion to your prompt plus some other information.  Here's how to access\n",
    "  # just the text of the completion.\n",
    "  return response.choices[0].message.content.strip()\n",
    "\n",
    "topic = \"cellular respiration\"\n",
    "generate_subject_few_shot(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wmr06VEzxlnr"
   },
   "source": [
    "That's it!  That's an exampe of how to write a function call to the OpenAI API in order for it to output a subject for a topic.\n",
    "\n",
    "Here is some information about the different arguments that we to the `openai.Completion.create` call:\n",
    " * `model` – OpenAI offers many different versions of their chat model. We'll use GPT3.5-turbo, which is functional without being [the most expensive to run](https://openai.com/api/pricing/).\n",
    " * `messages` - this is the set of messages that will instruct how the model should respond. The first message is a system prompt—an overall setting that specifies how the model should behave. In this case, the second message is the \"training set\" of examples followed by the last topic that we want to generate a subject for.\n",
    " * `temperature` - controls how much of the probability distribution the model will use when it is generating each token. 1.0 means that it samples from the complete probability distrubiton, 0.7 means that it drops the bottom 30% of the least likely tokens when it is sampling. 0.0 means that it will perform deterministically and always output the single most probable token for each context.\n",
    " * `top_p` - is an alternative way of controling the sampling.\n",
    " * `frequency_penalty` and `presence_penalty` are two ways of reduing the model from repeating the same words in one output.  You can set these to be >0 if you're seeing a lot of repetition in your output.\n",
    " * `max_tokens` is the maximum length in tokens that will be output by calling the function.  A token is a subword unit.  There are roughly 2 or 3 tokens per word on average.\n",
    " * `stop` is a list of stop sequences.  The model will stop generating output once it generates one of these strings, even if it hasn't reached the max token length. By default this is set to a special token `<|endoftext|>`.\n",
    "\n",
    "You can read more about [the Chat Completion API call in the documentation](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5iKKme91RMM"
   },
   "source": [
    "# Zero shot learning\n",
    "\n",
    "In addition to few shot learning, GPT can sometimes also perform \"zero shot learning\" where instead of giving it several examples of what we want it to do, we can instead give it instructions of what we want it to do.\n",
    "\n",
    "For example, for our topic - subject task we could give GPT the prompt\n",
    "\n",
    "> Given a topic, output the subject that a student should study if they want to know more about that topic.\n",
    "\n",
    "Then if we append\n",
    "> cellular respiration -\n",
    "\n",
    "GPT will output `biology`—or something similar. Since there is no training, you might have to accept slight variability in the output. `biology` and `biochemistry` are both reasonable outputs for the input `cellular respiration -`.\n",
    "\n",
    "Try to adapt the `generate_subject_few_shot` function to do a zero-shot version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aLve17Cl3d2m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Biology'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_subject_zero_shot(topic):\n",
    "  # TODO - write this function\n",
    "  zero_shot_prompt = \"\"\"Given a topic, output the subject that a student should study if they want to know more about that topic.\n",
    "\"\"\"\n",
    "\n",
    "  # All GPT completions are generated by the chat model now. The chat model works best\n",
    "  # when it's given a baseline instruction about how GPT should behave. We'll use this one.\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"Give a single subject\"}\n",
    "\n",
    "  client = openai.OpenAI()\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages= [\n",
    "          system_message,\n",
    "          {\"role\" : \"user\", \"content\" : zero_shot_prompt + topic + \" - \"} # simulate a user prompt\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"\\n\"]\n",
    "  )\n",
    "  # I recommend putting a short wait after each call,\n",
    "  # since the rate limit for the platform is 60 requests/min.\n",
    "  # (This increases to 3000 requests/min after you've been using the platform for 2 days).\n",
    "  time.sleep(1)\n",
    "\n",
    "  # the response from OpenAI's API is a JSON object that contains\n",
    "  # the completion to your prompt plus some other information.  Here's how to access\n",
    "  # just the text of the completion.\n",
    "  return response.choices[0].message.content.strip()\n",
    "\n",
    "topic = \"cellular respiration\"\n",
    "generate_subject_zero_shot(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3eB1xva5Nr0"
   },
   "source": [
    "A very cool recent finding is that training procedure for LLMs can be changed to improve this instruction following behavior.  If LLMs are [trained to do multiple tasks through prompting](https://arxiv.org/abs/2110.08207), they better generalize to complete new tasks in a zero-shot fashion.\n",
    "\n",
    "Try writing zero-shot prompts to do the following tasks:\n",
    "1. Summarize a Wikipedia article.\n",
    "2. Answer questions about an article.\n",
    "3. Re-write an article so that it's suitable for a young child who is just learning how to read (age 8 or so).\n",
    "4. Translate an article from Russian into English.\n",
    "\n",
    "You should experiment with a few prompts in the playground to find a good prompt that seems to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "g5AWEh526gIw"
   },
   "outputs": [],
   "source": [
    "def summarize(article_text):\n",
    "  # TODO - write this function\n",
    "  zero_shot_prompt = \"\"\"Given an article, output a short summary of it in enlgish.\n",
    "                      \"\"\"\n",
    "\n",
    "  # All GPT completions are generated by the chat model now. The chat model works best\n",
    "  # when it's given a baseline instruction about how GPT should behave. We'll use this one.\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"Focus on the subject of the article\"}\n",
    "\n",
    "  client = openai.OpenAI()\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages= [\n",
    "          system_message,\n",
    "          {\"role\" : \"user\", \"content\" : zero_shot_prompt + article_text + \" - \"} # simulate a user prompt\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"\\n\"]\n",
    "  )\n",
    "  \n",
    "  time.sleep(1)\n",
    "\n",
    "  # the response from OpenAI's API is a JSON object that contains\n",
    "  # the completion to your prompt plus some other information.  Here's how to access\n",
    "  # just the text of the completion.\n",
    "  summary = response.choices[0].message.content.strip()\n",
    "  return summary\n",
    "\n",
    "def answer_question(article_text, question):\n",
    "  # TODO - write this function\n",
    "  zero_shot_prompt = \"\"\"Given an article, answer the question asked.\n",
    "                      \"\"\"\n",
    "\n",
    "  # All GPT completions are generated by the chat model now. The chat model works best\n",
    "  # when it's given a baseline instruction about how GPT should behave. We'll use this one.\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"Answer the question asked\"}\n",
    "\n",
    "  client = openai.OpenAI()\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages= [\n",
    "          system_message,\n",
    "          {\"role\" : \"user\", \"content\" : zero_shot_prompt + article_text + question + \" - \"} # simulate a user prompt\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"\\n\"]\n",
    "  )\n",
    "  \n",
    "  time.sleep(1)\n",
    "\n",
    "  # the response from OpenAI's API is a JSON object that contains\n",
    "  # the completion to your prompt plus some other information.  Here's how to access\n",
    "  # just the text of the completion.\n",
    "  summary = response.choices[0].message.content.strip()\n",
    "  return summary\n",
    "  answer = \"\"\n",
    "  return answer\n",
    "\n",
    "def simplify(article_text):\n",
    "  # TODO - write a function to re-write an article so that it's suitable for a young child.\n",
    "  zero_shot_prompt = \"\"\"Given an article, output a short summary of it in enlgish at a child's reading level.\n",
    "                      \"\"\"\n",
    "\n",
    "  # All GPT completions are generated by the chat model now. The chat model works best\n",
    "  # when it's given a baseline instruction about how GPT should behave. We'll use this one.\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"Make sure the summary is at a child's reading level\"}\n",
    "\n",
    "  client = openai.OpenAI()\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages= [\n",
    "          system_message,\n",
    "          {\"role\" : \"user\", \"content\" : zero_shot_prompt + article_text + \" - \"} # simulate a user prompt\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"\\n\"]\n",
    "  )\n",
    "  \n",
    "  time.sleep(1)\n",
    "\n",
    "  # the response from OpenAI's API is a JSON object that contains\n",
    "  # the completion to your prompt plus some other information.  Here's how to access\n",
    "  # just the text of the completion.\n",
    "  summary = response.choices[0].message.content.strip()\n",
    "  return summary\n",
    "\n",
    "def translate(article_text, source_language, target_language):\n",
    "  # TODO - write a function to translate an article from a source language to a target language.\n",
    "  zero_shot_prompt = \"Translate an article from \" + source_language + \" to \" + target_language + article_text + \".\"\n",
    "\n",
    "  # All GPT completions are generated by the chat model now. The chat model works best\n",
    "  # when it's given a baseline instruction about how GPT should behave. We'll use this one.\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"Follow the prompt\"}\n",
    "\n",
    "  client = openai.OpenAI()\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages= [\n",
    "          system_message,\n",
    "          {\"role\" : \"user\", \"content\" : zero_shot_prompt + article_text + \" - \"} # simulate a user prompt\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"\\n\"]\n",
    "  )\n",
    "  \n",
    "  time.sleep(1)\n",
    "\n",
    "  # the response from OpenAI's API is a JSON object that contains\n",
    "  # the completion to your prompt plus some other information.  Here's how to access\n",
    "  # just the text of the completion.\n",
    "  summary = response.choices[0].message.content.strip()\n",
    "  return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9oCIWFW7-Bl"
   },
   "source": [
    "Show your outputs in your prompts.  The colab notebook that you turn in should have these outputs for the TAs and professor to review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "y8ODUSvg8dbE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article provides an overview of Formula One (F1), the highest class of international racing for open-wheel single-seater formula cars. It discusses the history of Formula One, the regulations, the championships for drivers and constructors, as well as the financial and political aspects of the sport. The article also mentions the evolution of Formula One cars, the introduction of various technological advancements, and notable drivers such as Juan Manuel Fangio and Stirling Moss.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text = \"\"\"\n",
    "Formula One, commonly known as Formula 1 or F1, is the highest class of international racing for open-wheel single-seater formula racing cars sanctioned by the Fédération Internationale de l'Automobile (FIA). The FIA Formula One World Championship has been one of the world's premier forms of motorsport since its inaugural running in 1950. The word formula in the name refers to the set of rules all participants' cars must follow. A Formula One season consists of a series of races, known as Grands Prix. Grands Prix take place in multiple countries and continents on either purpose-built circuits or closed roads.\n",
    "A point-system is used at Grands Prix to determine two annual World Championships: one for the drivers, and one for the constructors (the teams). Each driver must hold a valid Super Licence, the highest class of racing licence the FIA issues, and the races must be held on grade one tracks, the highest grade rating the FIA issues for tracks.\n",
    "Formula One cars are the world's fastest regulated road-course racing cars, owing to high cornering speeds achieved by generating large amounts of aerodynamic downforce, much of which is generated by front and rear wings. The cars depend on electronics, aerodynamics, suspension, and tyres. Traction control, launch control, and automatic shifting, and other electronic driving aids were first banned in 1994. They were briefly reintroduced in 2001, and have more recently been banned since 2004 and 2008, respectively.[1]\n",
    "With the average annual cost of running a team—designing, building, and maintaining cars, pay, transport—at approximately £220,000,000 (or $265,000,000),[2] Formula One's financial and political battles are widely reported. The Formula One Group is owned by Liberty Media, which acquired it in 2017 from private-equity firm CVC Capital Partners for £6.4 billion ($8 billion).[3][4]\n",
    "Formula One originated from the World Manufacturers' Championship (1925–1930) and European Drivers' Championship (1931–1939). The formula is a set of rules that all participants' cars must follow. Formula One was a formula agreed upon in 1946 to officially become effective in 1947. The first Grand Prix in accordance with the new regulations was the 1946 Turin Grand Prix, anticipating the formula's official start.[5][6] Before World War II, a number of Grand Prix racing organisations made suggestions for a new championship to replace the European Championship, but due to the suspension of racing during the conflict, the new International Formula for cars did not become formalised until 1946, to become effective in 1947. The new World Championship was instituted to commence in 1950.[7]\n",
    "The first world championship race, the 1950 British Grand Prix, took place at Silverstone Circuit in the United Kingdom on 13 May 1950.[8] Giuseppe Farina, competing for Alfa Romeo, won the first Drivers' World Championship, narrowly defeating his teammate Juan Manuel Fangio. Fangio won the championship in 1951, 1954, 1955, 1956, and 1957.[9] This set the record for the most World Championships won by a single driver, a record that stood for 46 years until Michael Schumacher won his sixth championship in 2003.[9]\n",
    "Juan Manuel Fangio's 1951 title-winning Alfa Romeo 159\n",
    "A Constructors' Championship was added in the 1958 season. Stirling Moss, despite being regarded as one of the greatest Formula One drivers in the 1950s and 1960s, never won the Formula One championship.[10] Between 1955 and 1961, Moss finished second in the championship four times and third the other three times.[11][12] Fangio won 24 of the 52 races he entered—still the record for the highest Formula One winning percentage by an individual driver.[13] National championships existed in South Africa and the UK in the 1960s and 1970s. Promoters held non-championship Formula One events for many years. Due to the increasing cost of competition, the last of these was held in 1983.[14][15]\n",
    "This era featured teams managed by road-car manufacturers, such as Alfa Romeo, Ferrari, Mercedes-Benz and Maserati. The first seasons featured prewar cars like Alfa Romeo's 158, which were front-engined, with narrow tyres and 1.5-litre supercharged or 4.5-litre naturally aspirated engines. The 1952 and 1953 seasons were run to Formula Two regulations, for smaller, less powerful cars, due to concerns over the dearth of Formula One cars.[16][17] When a new Formula One formula for engines limited to 2.5 litres was reinstated for the 1954 world championship, Mercedes-Benz introduced its W196, which featured things never seen on Formula One cars before, such as desmodromic valves, fuel injection, and enclosed streamlined bodywork. Mercedes drivers won the championship for the next two years, before the team withdrew from all motorsport competitions due to the 1955 Le Mans disaster.[18][19]\n",
    "\"\"\"\n",
    "\n",
    "summarize(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "N6Y3--nI8h-b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who owns formula one?\n",
      "Formula One is owned by Liberty Media, which acquired it in 2017 from private-equity firm CVC Capital Partners for £6.4 billion ($8 billion).\n",
      "---\n",
      "What is the average annual cost of running a Formula One team?\n",
      "The average annual cost of running a Formula One team is approximately £220,000,000 or $265,000,000.\n",
      "---\n",
      "What year was formula one first started?\n",
      "Formula One was first started in 1950.\n",
      "---\n",
      "Where did the first race take place?\n",
      "The first race took place at the 1950 British Grand Prix, held at the Silverstone Circuit in the United Kingdom on 13 May 1950.\n",
      "---\n",
      "What is a current team on the grid today?\n",
      "One of the current teams on the Formula One grid today is Mercedes-AMG Petronas Formula One Team.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "article_text = \"\"\"\n",
    "Formula One, commonly known as Formula 1 or F1, is the highest class of international racing for open-wheel single-seater formula racing cars sanctioned by the Fédération Internationale de l'Automobile (FIA). The FIA Formula One World Championship has been one of the world's premier forms of motorsport since its inaugural running in 1950. The word formula in the name refers to the set of rules all participants' cars must follow. A Formula One season consists of a series of races, known as Grands Prix. Grands Prix take place in multiple countries and continents on either purpose-built circuits or closed roads.\n",
    "A point-system is used at Grands Prix to determine two annual World Championships: one for the drivers, and one for the constructors (the teams). Each driver must hold a valid Super Licence, the highest class of racing licence the FIA issues, and the races must be held on grade one tracks, the highest grade rating the FIA issues for tracks.\n",
    "Formula One cars are the world's fastest regulated road-course racing cars, owing to high cornering speeds achieved by generating large amounts of aerodynamic downforce, much of which is generated by front and rear wings. The cars depend on electronics, aerodynamics, suspension, and tyres. Traction control, launch control, and automatic shifting, and other electronic driving aids were first banned in 1994. They were briefly reintroduced in 2001, and have more recently been banned since 2004 and 2008, respectively.[1]\n",
    "With the average annual cost of running a team—designing, building, and maintaining cars, pay, transport—at approximately £220,000,000 (or $265,000,000),[2] Formula One's financial and political battles are widely reported. The Formula One Group is owned by Liberty Media, which acquired it in 2017 from private-equity firm CVC Capital Partners for £6.4 billion ($8 billion).[3][4]\n",
    "Formula One originated from the World Manufacturers' Championship (1925–1930) and European Drivers' Championship (1931–1939). The formula is a set of rules that all participants' cars must follow. Formula One was a formula agreed upon in 1946 to officially become effective in 1947. The first Grand Prix in accordance with the new regulations was the 1946 Turin Grand Prix, anticipating the formula's official start.[5][6] Before World War II, a number of Grand Prix racing organisations made suggestions for a new championship to replace the European Championship, but due to the suspension of racing during the conflict, the new International Formula for cars did not become formalised until 1946, to become effective in 1947. The new World Championship was instituted to commence in 1950.[7]\n",
    "The first world championship race, the 1950 British Grand Prix, took place at Silverstone Circuit in the United Kingdom on 13 May 1950.[8] Giuseppe Farina, competing for Alfa Romeo, won the first Drivers' World Championship, narrowly defeating his teammate Juan Manuel Fangio. Fangio won the championship in 1951, 1954, 1955, 1956, and 1957.[9] This set the record for the most World Championships won by a single driver, a record that stood for 46 years until Michael Schumacher won his sixth championship in 2003.[9]\n",
    "Juan Manuel Fangio's 1951 title-winning Alfa Romeo 159\n",
    "A Constructors' Championship was added in the 1958 season. Stirling Moss, despite being regarded as one of the greatest Formula One drivers in the 1950s and 1960s, never won the Formula One championship.[10] Between 1955 and 1961, Moss finished second in the championship four times and third the other three times.[11][12] Fangio won 24 of the 52 races he entered—still the record for the highest Formula One winning percentage by an individual driver.[13] National championships existed in South Africa and the UK in the 1960s and 1970s. Promoters held non-championship Formula One events for many years. Due to the increasing cost of competition, the last of these was held in 1983.[14][15]\n",
    "This era featured teams managed by road-car manufacturers, such as Alfa Romeo, Ferrari, Mercedes-Benz and Maserati. The first seasons featured prewar cars like Alfa Romeo's 158, which were front-engined, with narrow tyres and 1.5-litre supercharged or 4.5-litre naturally aspirated engines. The 1952 and 1953 seasons were run to Formula Two regulations, for smaller, less powerful cars, due to concerns over the dearth of Formula One cars.[16][17] When a new Formula One formula for engines limited to 2.5 litres was reinstated for the 1954 world championship, Mercedes-Benz introduced its W196, which featured things never seen on Formula One cars before, such as desmodromic valves, fuel injection, and enclosed streamlined bodywork. Mercedes drivers won the championship for the next two years, before the team withdrew from all motorsport competitions due to the 1955 Le Mans disaster.[18][19]\n",
    "\"\"\"\n",
    "questions = [\n",
    "    \"Who owns formula one?\",\n",
    "    \"What is the average annual cost of running a Formula One team?\",\n",
    "    \"What year was formula one first started?\",\n",
    "    \"Where did the first race take place?\",\n",
    "    \"What is a current team on the grid today?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "  answer = answer_question(article_text, question)\n",
    "  print(question)\n",
    "  print(answer)\n",
    "  print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5d2SS-fT8klT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Formula One, also known as F1, is a really fast and exciting racing sport. Drivers race in special cars on cool tracks all around the world. They have to follow certain rules called \"formula.\" Each year, the drivers and the teams that make the cars compete to win championships. The cars are super fast and use special technology to go even faster. The sport has been around for a long time and has a lot of history. The races are fun to watch and the drivers are super talented.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text = \"\"\"\n",
    "Formula One, commonly known as Formula 1 or F1, is the highest class of international racing for open-wheel single-seater formula racing cars sanctioned by the Fédération Internationale de l'Automobile (FIA). The FIA Formula One World Championship has been one of the world's premier forms of motorsport since its inaugural running in 1950. The word formula in the name refers to the set of rules all participants' cars must follow. A Formula One season consists of a series of races, known as Grands Prix. Grands Prix take place in multiple countries and continents on either purpose-built circuits or closed roads.\n",
    "A point-system is used at Grands Prix to determine two annual World Championships: one for the drivers, and one for the constructors (the teams). Each driver must hold a valid Super Licence, the highest class of racing licence the FIA issues, and the races must be held on grade one tracks, the highest grade rating the FIA issues for tracks.\n",
    "Formula One cars are the world's fastest regulated road-course racing cars, owing to high cornering speeds achieved by generating large amounts of aerodynamic downforce, much of which is generated by front and rear wings. The cars depend on electronics, aerodynamics, suspension, and tyres. Traction control, launch control, and automatic shifting, and other electronic driving aids were first banned in 1994. They were briefly reintroduced in 2001, and have more recently been banned since 2004 and 2008, respectively.[1]\n",
    "With the average annual cost of running a team—designing, building, and maintaining cars, pay, transport—at approximately £220,000,000 (or $265,000,000),[2] Formula One's financial and political battles are widely reported. The Formula One Group is owned by Liberty Media, which acquired it in 2017 from private-equity firm CVC Capital Partners for £6.4 billion ($8 billion).[3][4]\n",
    "Formula One originated from the World Manufacturers' Championship (1925–1930) and European Drivers' Championship (1931–1939). The formula is a set of rules that all participants' cars must follow. Formula One was a formula agreed upon in 1946 to officially become effective in 1947. The first Grand Prix in accordance with the new regulations was the 1946 Turin Grand Prix, anticipating the formula's official start.[5][6] Before World War II, a number of Grand Prix racing organisations made suggestions for a new championship to replace the European Championship, but due to the suspension of racing during the conflict, the new International Formula for cars did not become formalised until 1946, to become effective in 1947. The new World Championship was instituted to commence in 1950.[7]\n",
    "The first world championship race, the 1950 British Grand Prix, took place at Silverstone Circuit in the United Kingdom on 13 May 1950.[8] Giuseppe Farina, competing for Alfa Romeo, won the first Drivers' World Championship, narrowly defeating his teammate Juan Manuel Fangio. Fangio won the championship in 1951, 1954, 1955, 1956, and 1957.[9] This set the record for the most World Championships won by a single driver, a record that stood for 46 years until Michael Schumacher won his sixth championship in 2003.[9]\n",
    "Juan Manuel Fangio's 1951 title-winning Alfa Romeo 159\n",
    "A Constructors' Championship was added in the 1958 season. Stirling Moss, despite being regarded as one of the greatest Formula One drivers in the 1950s and 1960s, never won the Formula One championship.[10] Between 1955 and 1961, Moss finished second in the championship four times and third the other three times.[11][12] Fangio won 24 of the 52 races he entered—still the record for the highest Formula One winning percentage by an individual driver.[13] National championships existed in South Africa and the UK in the 1960s and 1970s. Promoters held non-championship Formula One events for many years. Due to the increasing cost of competition, the last of these was held in 1983.[14][15]\n",
    "This era featured teams managed by road-car manufacturers, such as Alfa Romeo, Ferrari, Mercedes-Benz and Maserati. The first seasons featured prewar cars like Alfa Romeo's 158, which were front-engined, with narrow tyres and 1.5-litre supercharged or 4.5-litre naturally aspirated engines. The 1952 and 1953 seasons were run to Formula Two regulations, for smaller, less powerful cars, due to concerns over the dearth of Formula One cars.[16][17] When a new Formula One formula for engines limited to 2.5 litres was reinstated for the 1954 world championship, Mercedes-Benz introduced its W196, which featured things never seen on Formula One cars before, such as desmodromic valves, fuel injection, and enclosed streamlined bodywork. Mercedes drivers won the championship for the next two years, before the team withdrew from all motorsport competitions due to the 1955 Le Mans disaster.[18][19]\n",
    "\"\"\"\n",
    "\n",
    "simplify(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "w8f1cGZN8nX4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The elected President of the United States, Donald Trump, has stated that he may impose 100% tariffs against BRICS countries if they do not abandon plans to create a unified currency as an alternative to the dollar.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russian_article = \"\"\"\n",
    "Избранный президент США Дональд Трамп заявил, что может ввести стопроцентные пошлины против стран БРИКС, если они не откажутся от планов по созданию единой валюты объединения в качестве альтернативы доллару.\n",
    "\"Мы требуем от этих стран обязательства не создавать новую валюту БРИКС\", - написал Трамп в социальной сети Truth Social.\n",
    "Он также потребовал, что бы страны БРИКС не поддерживали никакую другую валюту, кроме \"могущественного доллара США\".\n",
    "\"Иначе они столкнутся с 100% пошлинами\", - пригрозил Трамп, добавив, что для этих стран может быть ограничена торговля с США.\n",
    "\"\"\"\n",
    "\n",
    "source_language = \"Russian\"\n",
    "target_language = \"English\"\n",
    "translate(russian_article, source_language, target_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVe3k3sl9p6K"
   },
   "source": [
    "## TODO - Pick your own task\n",
    "\n",
    "For this section you should pick some task that you'd like to have GPT do.  Add a description and code to your notebook here.  You should:\n",
    "1. Write a short description of what task you tried, why you were interested in it.\n",
    "2. Give some code so that we can reproduce what you did via an Open API call.  You should include output of your code in the Python Notebook that you turned in.\n",
    "3. Write a short qualitative analysis of whether or not GPT did the task well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7uWoojg-YY-"
   },
   "source": [
    "TODO - your task description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "FgwbDBsS-a2I"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The top 5 all-time players in professional football are Tom Brady, Jerry Rice, Lawrence Taylor, Jim Brown, and Reggie White.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO your code\n",
    "def top_five(sport):\n",
    "  zero_shot_prompt = \"Give the top 5 all time players in the professional sport of \" + sport + \".\"\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"Output this in a single sentence. Adhere to american sport labels.\"}\n",
    "\n",
    "  client = openai.OpenAI()\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages= [\n",
    "          system_message,\n",
    "          {\"role\" : \"user\", \"content\" : zero_shot_prompt + sport + \" - \"} # simulate a user prompt\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"\\n\"]\n",
    "  )\n",
    "  \n",
    "  time.sleep(1)\n",
    "\n",
    "  \n",
    "  summary = response.choices[0].message.content.strip()\n",
    "  return summary\n",
    "\n",
    "sport = \"football\"\n",
    "top_five(sport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9CjM8hM-cc2"
   },
   "source": [
    "TODO - write a short paragraph giving your qualitative analysis of how well GPT did for your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the short paragraph explaining my code\n",
    "\"\"\"This goal of this is to input a sport that exists in the world and have the top 5 players outputed. This is a slightly subjective subject, so the results will vary from case to case.\n",
    "The function outputs are acceptable in that they stay consistent and when given a sport that has alternative names, it adheres to the standard of the US-labeling system. This is instructed\n",
    "to output the answer in a single sentence, but it can be altered to output it line by line instead, giving a more list like aesthetic.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7x8z9JJnAQs5"
   },
   "source": [
    "# Fine Tuning\n",
    "\n",
    "In addition to zero-shot and few-shot learning, another way of getting large language models to do your tasks is via a process called \"fine tuning\".  In fine-tuning the model updates its parameters so that it performs well on many training examples.  The training examples are in the form of input prompts paired with gold standard completions.\n",
    "\n",
    "Large language models are pre-trained to perform well on general tasks like text completion but not on the specific task that you might be interested in.  The models can be fine tuned to perform you task, starting with the model parameters that are good for the general setting, and then updating them to be good for your task.\n",
    "\n",
    "We'll walk through how to fine-tune GPT for a task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDez1YvZHFaf"
   },
   "source": [
    "For this example, we will show you how to fine tune GPT to write biographies. From data in the info boxes in Wikipedia pages.  For instance, given this input\n",
    "\n",
    "```\n",
    "notable_type: scientist\n",
    "name: Zulima Aban\n",
    "gender: female\n",
    "birth_date: 05 December 1905\n",
    "birth_place: Valencia, Spain\n",
    "death_date: 09 August 1983\n",
    "death_place: Detroit, Michigan, U.S.\n",
    "death_cause: Pulmonary embolism\n",
    "occupation: Astronomer\n",
    "fields: Astrophysics, Computer Science, Computer Graphics, Interface Design, Image Synthesis\n",
    "known_for: The Search for Planet Nine\n",
    "hometown: Detroit, Michigan, U.S.\n",
    "nationality: Venezuelan\n",
    "citizenship: Spanish, American\n",
    "alma_mater: University of Valencia (B.Sc.), University of Madrid (Ph.D.)\n",
    "thesis_title: The Formation of Planets by the Accretion of Small Particles\n",
    "thesis_year: 1956\n",
    "doctoral_advisor: Angela Carter\n",
    "awards: Spanish Academy of Science, Spanish Academy of Engineering, German Aerospace Prize, IEEE Medal of Honor, IEEE John von Neumann Medal, IEEE Jack S. Kilby Signal Processing Medal, United Nations Space Pioneer Award, Wolf Prize in Physics\n",
    "institutions: Oberlin College, University of Valencia, Instituto de Astrofísica de Andalucía (CSIC), University of Southern California, Space Telescope Science Institute (STScI)\n",
    "notable_students: Ryan Walls\n",
    "influences: Immanuel Kant, Albert Einstein, Kurt Gödel, Gottfried Leibniz, Richard Feynman, Werner Heisenberg, William Kingdon Clifford, Sir Arthur Eddington\n",
    "influenced: Joseph Weinberg\n",
    "mother: Ana Aban\n",
    "father: Joaquín Aban\n",
    "partner: Georgina Abbott\n",
    "children: Robert, Peter, Sarah\n",
    "```\n",
    "\n",
    "The fine-tuned model will generate this output:\n",
    "\n",
    "> Zulima Aban was a Venezuelan astronomer, who was born on 05 December 1905 in Valencia, Spain to Ana Aban and Joaquín Aban. Her career involved the fields of Astrophysics, Computer Science, Computer Graphics, Interface Design, Image Synthesis. Aban was known for The Search for Planet Nine. Aban went to University of Valencia (B.Sc.), University of Madrid (Ph.D.). Aban's thesis title was The Formation of Planets by the Accretion of Small Particles in 1956. Her doctoral advisor was Angela Carter. Aban received Spanish Academy of Science, Spanish Academy of Engineering, German Aerospace Prize, IEEE Medal of Honor, IEEE John von Neumann Medal, IEEE Jack S. Kilby Signal Processing Medal, United Nations Space Pioneer Award, Wolf Prize in Physics. Aban went to Oberlin College, University of Valencia, Instituto de Astrofísica de Andalucía (CSIC), University of Southern California, Space Telescope Science Institute (STScI). Her notable students were Ryan Walls. Aban was influenced by Immanuel Kant, Albert Einstein, Kurt Gödel, Gottfried Leibniz, Richard Feynman, Werner Heisenberg, William Kingdon Clifford, Sir Arthur Eddington and she infuenced Joseph Weinberg. Aban was married to Georgina Abbott and together had three children, Robert, Peter, Sarah. Aban died on 09 August 1983 in Detroit, Michigan, U.S due to Pulmonary embolism.\n",
    "\n",
    "The dataset that we will use was created for the paper [SynthBio: A Case Study in Human-AI Collaborative Curation of Text Datasets](https://www.cis.upenn.edu/~ccb/publications/synthbio.pdf) by Ann Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris Callison-Burch, Andy Coenen, and Sebastian Gehrmann. It was published in NeurIPS 2021.  The goal of the paper was to create a curated dataset for training large language models on synthetic data with the goal of avoiding the gender and geographic bias that is naturally present in Wikipedia due to cultural and historic reasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1WbRlYDIadg"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "5mrmOrl6Ad0E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 5671k  100 5671k    0     0  6228k      0 --:--:-- --:--:-- --:--:-- 6225k\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_train.json\n",
    "\n",
    "# If you're running this locally on a Mac, you can use the following command instead.\n",
    "!curl -o SynthBio_train.json https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_train.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "RFXcmRh-Chll",
    "outputId": "682191bc-8b2f-4b97-cd9a-057e6efab3e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notable_type: theologian\n",
      "name: Ibrahim Al-Ayoub\n",
      "gender: female\n",
      "nationality: Jordanian\n",
      "birth_date: 27 July 1898\n",
      "birth_place: Tafila, Jordan\n",
      "death_date: 1 March, 1989\n",
      "death_place: Amman, Jordan\n",
      "death_cause: stroke\n",
      "alma_mater: unknown\n",
      "occupation: writer, philosopher\n",
      "notable_works: A Prophet's Cry, Wisdom for Life, Life and Living, The Tree with a Thousand Souls\n",
      "main_interests: Eastern Orthodoxy, philosophy, self-transcendence\n",
      "mother: Al-Ayoubi\n",
      "father: Al-Ayoubi\n",
      "partner: Issa Al-Boustani\n",
      "children: none\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ibrahim Al-Ayoub (born 27 July 1898 in Tafila, Jordan - died 1 March 1989 in Amman, Jordan) was a Jordanian writer, philosopher. She was best known for his book \"A Prophet\\'s Cry\", which detailed his thoughts on the life and suffering of Jesus. She died in Amman, Jordan. She is the daughter of Al-Ayoubi and Al-Ayoubi.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a file called 'SynthBio.json' which is a list of json objects.\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "def load_wiki_bio_data(filename='SynthBio_train.json', num_bios=100, randomized=True):\n",
    "  with open(filename) as f:\n",
    "    synth_bio_data = json.load(f)\n",
    "  random.shuffle(synth_bio_data)\n",
    "  bios = []\n",
    "  for data in synth_bio_data:\n",
    "    notable_type = data['notable_type']\n",
    "    attributes = \"notable_type: {notable_type} | {other_attributes}\".format(\n",
    "        notable_type = notable_type,\n",
    "        other_attributes = data['serialized_attrs']\n",
    "    )\n",
    "    biography = data['biographies'][0]\n",
    "    bios.append((attributes.replace(\" | \", \"\\n\"), biography))\n",
    "  return bios[:min(num_bios, len(bios))]\n",
    "\n",
    "wiki_bios = load_wiki_bio_data()\n",
    "attributes, bio = wiki_bios[0]\n",
    "print(attributes)\n",
    "print('---')\n",
    "bio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDZYB6CW2m7C"
   },
   "source": [
    "## Format Data for Fine-Tuning\n",
    "\n",
    "Below, I show how to format data to fine-tune OpenAI.  The OpenAI API documentation has a [guide to fine-tuning models](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset) that you should read.   The basic format of fine-tuning data is a JSONL file (one JSON object per line) with an entry for `\"messages\"` that itself contains a list of `message` objects. Each `message` object consists of an entry for a `role` and `content`.\n",
    "\n",
    "```\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Explanation for how LLM should behave.\"}, {\"role\": \"user\", \"content\": \"User's prompt 1\"}, {\"role\": \"assistant\", \"content\": \"Expected response 1.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Explanation for how LLM should behave.\"}, {\"role\": \"user\", \"content\": \"User's prompt 2\"}, {\"role\": \"assistant\", \"content\": \"Expected response 2.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Explanation for how LLM should behave.\"}, {\"role\": \"user\", \"content\": \"User's prompt 3\"}, {\"role\": \"assistant\", \"content\": \"Expected response 3.\"}]}\n",
    "...\n",
    "```\n",
    "\n",
    "In the code below, I'll extract a prompt that contains the `attributes` variable from the intent determination data, and I'll have the completion be the `biography` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "2UKlNc01b4LR"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_wikibio_finetuning_data(wikibios, fine_tuning_filename):\n",
    "  fine_tuning_data = []\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"You should generate a biography for a person based on the details provided.\"}\n",
    "\n",
    "  for attributes, bio in wikibios:\n",
    "    prompt = \"{attributes}\\n---\\n\".format(attributes=attributes)\n",
    "    completion = \"Biography: {bio}\\n###\".format(bio=bio)\n",
    "    data = {\"messages\" : [system_message, {\"role\" : \"user\", \"content\" : prompt}, {\"role\" : \"assistant\", \"content\" : completion}]}\n",
    "    fine_tuning_data.append(data)\n",
    "\n",
    "  random.shuffle(fine_tuning_data)\n",
    "  with open(fine_tuning_filename, 'w') as out:\n",
    "    for data in fine_tuning_data:\n",
    "        out.write(json.dumps(data))\n",
    "        out.write('\\n')\n",
    "\n",
    "\n",
    "fine_tuning_filename='wikibio_finetuning_data.jsonl'\n",
    "create_wikibio_finetuning_data(wiki_bios, fine_tuning_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEqja42Yc5O3"
   },
   "source": [
    "Next, we'll perform fine-tuning with this data using OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "og19yX-Mc4-i"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install --upgrade openai\n",
    "%pip install jsonlines\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fE8RjE6SdGGk"
   },
   "source": [
    "Once you've got access to the OpenAI API, you can find your OpenAI API key [here](https://beta.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "g2uAKwEzdGrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from getpass import getpass\n",
    "print('Enter OpenAI API key:')\n",
    "openai.api_key = getpass()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9SVG2fudLK9",
    "outputId": "c8451a4a-4246-4b76-8402-32f4deb23e36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You should generate a biography for a person based on the details provided.\"}, {\"role\": \"user\", \"content\": \"notable_type: musician\\nname: Simon Nda'Okongo\\nbirth_name: Simon Nda'Okongo\\nalias: B'June\\ngender: non-binary\\nbirth_date: 27 August 1916\\nbirth_place: Libreville, Gabon\\ndeath_date: 1 January 1982\\ndeath_place: Libreville, Gabon\\ndeath_cause: complications due to pneumonia\\nresting_place: Mortuary of the Saint-Pierre-et-Saint-Paul Cathedral, Libreville\\ninstrument: drums\\ngenre: folktronica\\nhometown: Libreville, Gabon\\nnationality: Gabonese\\ncitizenship: British and American by descent\\neducation: University of California, Los Angeles - School of Arts and Architecture, New York University\\nyears_active: 1940-1950\\nlabel: Paramount Records\\nassociated_acts: John Coltrane, Miles Davis, Billy Strayhorn, Duke Ellington\\nmother: Eulalie Enowa Ekongo\\nfather: Marcel B. Nda'kongo\\npartner: Dorothy Pak\\nchildren: Brice and Tr\\u00e9sor Nda'kongo\\n---\\n\"}, {\"role\": \"assistant\", \"content\": \"Biography: Simon Nda'Okongo alias B'June was a Gabonese and a citizen of British and American descent. They studied at University of California, Los Angeles - School of Arts and Architecture, New York University and were active over a period of 1940-1950. June\\u2019s labels are Paramount Records, associated acts are John Coltrane, Miles Davis, Billy Strayhorn, Duke Ellington. B'June was born on 27 August 1916 at Libreville, Gabon to Eulalie Enowa Ekongo and Marcel B. Nda'kongo. He died on 1 January 1982 at Libreville, Gabon due to pneumonia. and was buried in the Mortuary of the Saint-Pierre-et-Saint-Paul Cathedral, Libreville. They survived with their partner Dorothy Pak and children Brice and Tr\\u00e9sor Nda'kongo. They use instruments like drums of the genre folktronica. Their hometown is Libreville, Gabon.\\n###\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You should generate a biography for a person based on the details provided.\"}, {\"role\": \"user\", \"content\": \"notable_type: spy\\nname: Htet San\\ngender: non-binary\\nnationality: Burmese\\nbirth_date: 30 March 1888\\nbirth_place: British Burma\\ndeath_date: 17 February 1982\\ndeath_place: Rangoon, Burma\\ndeath_cause: complications from stroke\\nresting_place: Kandawgyi, Burma\\nserviceyears: 1922-1968\\nknown_for: civil rights advocate, author \\\"The Glass Palace\\\", political prisoner\\nalma_mater: Hemingway's, Columbia University\\noccupation: journalism, political activist\\ncodename: Thar Sein\\nallegiance: United States of America\\nagency: Federal Bureau of Investigation\\noperation: Project Rebirth\\nmother: Htet Khaing\\nfather: Thaung Ko Hte\\npartner: Chit Su\\nchildren: Kyaing Kya, Myet Kya\\n---\\n\"}, {\"role\": \"assistant\", \"content\": \"Biography: Htet San was born on 30 March 1888 in British Burma to Htet Khaing and Thaung Ko Hte. San was known for civil rights advocate, author \\\"The Glass Palace\\\", political prisoner with the code name Thar Sein. San went to Hemingway's, Columbia University and worked with agency Federal Bureau of Investigation. San occupation was journalism, political activist and worked on Project Rebirth. San was married to Chit Su and together had two children, Kyaing Kya, Myet Kya. San was died on 17 February 1982 in Rangoon, Burma due to complications from stroke and remains were done in Kandawgyi, Burma.\\n###\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You should generate a biography for a person based on the details provided.\"}, {\"role\": \"user\", \"content\": \"notable_type: athlete\\nname: Nizami Todria\\ngender: female\\nnationality: Georgia\\nbirth_date: 30 August 1933\\nbirth_place: Tbilisi, Georgian Soviet Socialist Republic\\nsport: gymnastics\\ncountry: Romania\\nhometown: Tbilisi\\ncitizenship: Soviet Union\\neducation: Georgian State University (1955), Georgian State Institute of Physical Culture (1956), Georgian Institute of Physical Culture (1976)\\nevent: gymnastics\\nyears_active: 18.5 years\\nretired: 1995\\nheight: 5ft 5in\\nweight: 115lb\\nworlds: 10th all around at the 1963 World Artistic Gymnastics Championships\\nmother: Doina-Carmen V\\u0103r\\u0103\\u015fteanu-Lajoiu\\nfather: Mihail Lajoiu\\n---\\n\"}, {\"role\": \"assistant\", \"content\": \"Biography: Nizami Todria (born 30 August 1933) is a Georgian gymnast. Todria competed in the 10th all around at the 1963 World Artistic Gymnastics Championships in worlds. She is born in Tbilisi, Georgian Soviet Socialist Republic. She is citizen of Soviet union and completed her education from Georgian State University (1955), Georgian State Institute of Physical Culture (1956), Georgian Institute of Physical Culture (1976). Her active years are 18.5 years. Her retired age is 1995. Her height is 5ft 5in and weight is 115lb. She was the daughter of Doina-Carmen V\\u0103r\\u0103\\u015fteanu-Lajoiu and Mihail Lajoiu.\\n###\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You should generate a biography for a person based on the details provided.\"}, {\"role\": \"user\", \"content\": \"notable_type: artist\\nname: Werner von Trapp\\ngender: female\\nnationality: Austrian\\nbirth_date: 28 June 1896\\nbirth_place: Salzburg, Austria\\ndeath_date: 29 October 1947\\ndeath_place: Vermont, USA\\ndeath_cause: \\\"heart attack\\\"\\nresting_place: Trapp Family Lodge, Stowe, Vermont\\nknown_for: Trapp Family Singers; The Sound of Music\\nmovement: Austrian folk singing\\nalma_mater: \\\"Miss Nellie Long\\\" Miss Trapp; School of the Sacred Heart Nonnenwerth, Westfalen\\nawards: Legion of Honour by the French Government\\nelected: president of a musical group\\nmother: Agatha Whitehead\\nfather: Richard von Trapp\\npartner: George Ziegler\\nchildren: Rupert, George, Maria Agatha, Werner, Hedwig, Johanna\\n---\\n\"}, {\"role\": \"assistant\", \"content\": \"Biography: Werner Franz von Trapp (born 28 June 1896 in Salzburg, died 29 October 1987 in Vermont, USA) was a member of the Trapp Family Singers and born to Richard von Trapp, Agatha Whitehead von Trapp and married to George. She and her partner had six children. She attended the \\\"Miss Nellie Long\\\" Miss Trapp; School of the Sacred Heart Nonnenwerth, Westfalen and won the Legion of Honour award by the French Government. She was a member of musical group. She died on October 29, 1947 in Vermont after suffering a heart attack and buried in Trapp Family Lodge, Stowe, Vermont.\\n###\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You should generate a biography for a person based on the details provided.\"}, {\"role\": \"user\", \"content\": \"notable_type: spy\\nname: Mahamed Ashan\\ngender: non-binary\\nnationality: Sri Lankan\\nbirth_date: 27 March 1913\\nbirth_place: Matar, Sri Lanka\\ndeath_date: 9 July 1983\\ndeath_place: Kandy, Sri Lanka\\ndeath_cause: respiratory failure\\nserviceyears: 1948-1990\\nknown_for: Sri Lankan independence activist for non-binary people\\nalma_mater: Sri Lanka University\\noccupation: civil engineer\\ncodename: Aruni\\nallegiance: Sri Lanka\\nagency: non-governmental organization\\nmother: Kamala Ashan\\nfather: Jayaweera Kurukulasuriya\\n---\\n\"}, {\"role\": \"assistant\", \"content\": \"Biography: Mahamed Ashan was a Sri Lankan civil engineer who also worked as a spy for the Sri Lankan government under the codename \\\"Aruni\\\". Ashan was born on March 27, 1913 in Matar, Sri Lanka to Kamala Ashan and Jayaweera Kurukulasuriya. They attended Sri Lanka University. They began performing clandestine operations in 1948 and continued until their death. Ashan died on July 9, 1983 of respiratory failure while in Kandy, Sri Lanka.\\n###\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You should generate a biography for a person based on the details provided.\"}, {\"role\": \"user\", \"content\": \"notable_type: artist\\nname: Maxim Schmidli\\ngender: male\\nnationality: Swiss\\nbirth_date: 24 August 1900\\nbirth_place: Zurich, Switzerland\\ndeath_date: 24 May 1999\\ndeath_place: Zollikerberg, Switzerland\\ndeath_cause: renal failure\\nresting_place: Zollikerberg Cemetery\\nknown_for: Expressionist artist famous for painting industrial scenes and landscapes\\nnotable_works: Unfinished Industrial scene, In the Garden\\nmovement: Expressionism\\nalma_mater: University of Zurich 1922-1925, University of Munich 1926-1927, Bauhaus Weimar 1927-1928\\nawards: Grand prize for Fine Arts from Bavaria, Grand prize from the National Association of German Artists\\nelected: Member of the German Reich Chamber of Fine Arts and Director of the Arts\\nmother: Ida Schmidli\\nfather: Adolf Schmidli\\npartner: Clara Berger\\nchildren: Guenther Schmidli, Verena Schmidli, Peter Schmidli, Rosmarie Schmidli\\n---\\n\"}, {\"role\": \"assistant\", \"content\": \"Biography: Maxim Schmidli was born on August 24, 1900 in Zurich, Switzerland. His father, Adolf Schmidli and his mother Ida Schmidli. Schmidli was married to Clara Berger and children are Guenther Schmidli, Verena Schmidli, Peter Schmidli, Rosmarie Schmidli. He studied at the University of Zurich from 1918 to 1922, and then at the University of Munich from 1922 to 1926 and at the Bauhaus in Weimar from 1927 to 1928. Schmidli was best known for his industrial landscapes. Schmidli died on May 24, 1999 due to renal failure at Zollikerberg, Switzerland and laid at Zollikerberg Cemetery. His notable works are Unfinished Industrial scene, In the Garden and movement is Expressionism. Schmidli awards are Grand prize for Fine Arts from Bavaria, Grand prize from the National Association of German Artists. He elected as Member of the German Reich Chamber of Fine Arts and Director of the Arts.\\n###\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You should generate a biography for a person based on the details provided.\"}, {\"role\": \"user\", \"content\": \"notable_type: theologian\\nname: Ibrahim Al-Ayoub\\ngender: female\\nnationality: Jordanian\\nbirth_date: 27 July 1898\\nbirth_place: Tafila, Jordan\\ndeath_date: 1 March, 1989\\ndeath_place: Amman, Jordan\\ndeath_cause: stroke\\nalma_mater: unknown\\noccupation: writer, philosopher\\nnotable_works: A Prophet's Cry, Wisdom for Life, Life and Living, The Tree with a Thousand Souls\\nmain_interests: Eastern Orthodoxy, philosophy, self-transcendence\\nmother: Al-Ayoubi\\nfather: Al-Ayoubi\\npartner: Issa Al-Boustani\\nchildren: none\\n---\\n\"}, {\"role\": \"assistant\", \"content\": \"Biography: Ibrahim Al-Ayoub (born 27 July 1898 in Tafila, Jordan - died 1 March 1989 in Amman, Jordan) was a Jordanian writer, philosopher. She was best known for his book \\\"A Prophet's Cry\\\", which detailed his thoughts on the life and suffering of Jesus. She died in Amman, Jordan. She is the daughter of Al-Ayoubi and Al-Ayoubi.\\n###\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You should generate a biography for a person based on the details provided.\"}, {\"role\": \"user\", \"content\": \"notable_type: theologian\\nname: Heather Mason\\ngender: non-binary\\nnationality: American\\nbirth_date: 05 January 1897\\nbirth_place: Boston, Massachusetts\\ndeath_date: 28 May, 1965\\ndeath_place: Boston, Massachusetts\\ndeath_cause: throat cancer\\nresting_place: Pine Grove Cemetery\\nalma_mater: Harvard University\\noccupation: theologian, teacher\\ntradition_movement: Unitarian Universalist\\nmain_interests: theology, ethics, the body, sexuality, women's issues, transgender issues\\nmother: Joan Davis\\nfather: Albert Mason\\npartner: none\\nchildren: none\\n---\\n\"}, {\"role\": \"assistant\", \"content\": \"Biography: Heather Mason (05 January 1897 - 28 May 1965) was an American theologian, teacher born in Boston, Massachusetts to Joan Davis and Albert Mason. They went to Harvard university. They main interests were theology, ethics, the body, sexuality, women's issues and transgender issues. Mason was a Unitarian Universalist. Mason died of throat cancer on 28 May, 1965 in Boston, Massachusetts. Their remains are in Pine Grove Cemetery.\\n###\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You should generate a biography for a person based on the details provided.\"}, {\"role\": \"user\", \"content\": \"notable_type: scientist\\nname: Simone Leclerc\\ngender: non-binary\\nbirth_date: 19 December 1873\\nbirth_place: Paris, France\\ndeath_date: 16 September 1943\\ndeath_place: Saint-Aubin-sur-Mer, Calvados\\ndeath_cause: Guillain Barre Syndrome\\nresting_place: St. Aubin-sur-Mer Cemetery\\noccupation: physicist\\nfields: cosmic radiation, nuclear physics, neutrinos, high-energy physics, quantum theory, dark matter, antimatter, particle physics, string theory, unified field, supersymmetry, quantum mechanics\\nknown_for: the discovery of supersymmetry\\nhometown: Saint-Aubin-sur-Mer\\nnationality: French\\ncitizenship: French\\nalma_mater: Ecole Polytechnique, Paris, France (1906-1911)\\nthesis_title: Theoretical Treatment of the Nuclei Alpha and Beta\\nthesis_year: 1912\\ndoctoral_advisor: Pierre Curie\\nawards: Prix Pereire (1916), Chevalier de la L\\u00e9gion D'Honneur (1920), Prix Charles Veillon (1922), Prix Madame Wiener (Soci\\u00e9t\\u00e9 fran\\u00e7aise de physique)\\ninstitutions: Ecole Sup\\u00e9rieure de Physique, Paris, France (1911); Institut d'Etude et de Recherche Scientifiques de Cannes, France\\ninfluences: Pierre Curie, Marie Curie, Ernest Rutherford, Niels Bohr, Enrico Fermi, Albert Einstein, Arthur Compton, James Franck, Hideki Yukawa\\nmother: Gabrielle Leclerc\\nfather: Marc Leclerc\\npartner: Marianne Fiolet\\n---\\n\"}, {\"role\": \"assistant\", \"content\": \"Biography: Simone Leclerc (non-binary) (19 December 1873 - 16 September 1943) was a French physicist known for the discovery of supersymmetry. Leclerc was born on 19 December 1873 in Paris to Gabrielle Leclerc and Marc Leclerc. Leclerc studied at the Ecole Sup\\u00e9rieure de Physique, Paris, France (1911). Leclerc was a student of Pierre Curie for the thesis he attended in Ecole Ecole Polytechnique, Paris, France of the thesis \\\"Theoretical Treatment of the Nuclei Alpha and Beta in 1912. Leclerc was a professor of physics at the Institut d'Etude et de Recherche Scientifiques de Cannes. Leclerc was interested in the varous fields like cosmic radiation, nuclear physics, neutrinos, high-energy physics, quantum theory, dark matter, antimatter, particle physics, string theory, unified field, supersymmetry, quantum mechanics . In 1916, Leclerc was awarded the Prix Pereire by the French Academy of Sciences for their work on radioactivity. In 1920, Leclerc was awarded the Chevalier de la L\\u00e9gion D'Honneur. In 1922, Leclerc was awarded the Prix Charles Veillon. In 1923, Leclerc was awarded the Prix Madame Wiener (Soci\\u00e9t\\u00e9 fran\\u00e7aise de physique). They influences are Pierre Curie, Marie Curie, Ernest Rutherford, Niels Bohr, Enrico Fermi, Albert Einstein, Arthur Compton, James Franck, Hideki Yukawa.They was partner of Marianne Fiolet. Leclerc died of Guillain Barre Syndrome in Saint-Aubin-sur-Mer, Calvados, France on 16 September 1943. Leclerc is buried at St. Aubin-sur-Mer Cemetery.\\n###\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"You should generate a biography for a person based on the details provided.\"}, {\"role\": \"user\", \"content\": \"notable_type: theologian\\nname: Adana Mim\\u00e2ndre\\ngender: female\\nnationality: Congolese\\nbirth_date: 22 November 1856\\nbirth_place: Kongo\\ndeath_date: 25 April 1946\\ndeath_place: Paris, France\\nresting_place: \\u00c9glise Saint-Paul\\noccupation: professor\\ntradition_movement: Neo-gnosticism\\nnotable_works: The Gospel According to the Holy Virgin Mary, The Word of God\\nmain_interests: Christianity and gnosticism, early Christianity, Christian mysticism and ascetic, Biblical hermeneutics\\n---\\n\"}, {\"role\": \"assistant\", \"content\": \"Biography: Adana Mim\\u00e2ndre (22 November 1856 - 25 April 1946) was a Congolese professor and gnostic philosopher. Mim\\u00e2ndre was born in Kongo and died in Paris, France. Her resting place is in \\u00c9glise Saint-Paul. Her notable works include The Gospel According to the Holy Virgin Mary, The Word of God and she was interested in Christianity and gnosticism, Christianity, Christian mysticism and ascetic, Biblical hermeneutics.\\n###\"}]}\n"
     ]
    }
   ],
   "source": [
    "!head '{fine_tuning_filename}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZDGRBY5ixfs"
   },
   "source": [
    "## Run the fine-tuning API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzqdtSXzdXD1"
   },
   "source": [
    "Next, we'll make the fine tuning call using the Python API. First, we must upload the training data to OpenAI. Once we do that, we can use the `id` of the uploaded data to perform the fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJ9-kAe1dWRO",
    "outputId": "33b715ba-3830-4301-f553-3bed518571c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-BULADKjXbSjKvhVAW6E1rz', bytes=145976, created_at=1732997738, filename='wikibio_finetuning_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"wikibio_finetuning_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGh4KsKriIdK"
   },
   "source": [
    "Make sure to change the string used for your `training_file` to the `id` that was returned as a result of the previous step.\n",
    "\n",
    "**Do NOT repeatedly execute the following cell.**  It will cost you upwards of a dollar each time you run it. Run it once, and then move on to the next cell, where you can check its progress. The amount of time required for the fine-tune job to finish can be highly variable. Do not—again, **DO NOT**—repeatedly execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Ccy_VxSOiIdK",
    "outputId": "1810036b-ab48-4907-f5a4-5bd1913733b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-Vod0juyldT7zqUr8CLngUUZY', created_at=1732997793, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-CtG7bJ67coneBV3uvISitdeu', result_files=[], seed=1732171217, status='validating_files', trained_tokens=None, training_file='file-BULADKjXbSjKvhVAW6E1rz', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-BULADKjXbSjKvhVAW6E1rz\",\n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ8j8VRVdfv6"
   },
   "source": [
    "You should copy down the fine-tune's `id`.\n",
    "\n",
    "You can list all of your fine-tune jobs and check their statuses using the following few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "seMeIwOAdgcB",
    "outputId": "86b60f38-fc0e-4fe7-c88e-276822e4691e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-klODJjaWwUDkTSjpEtbkyoEv', created_at=1732997974, level='info', message='Step 15/300: training loss=0.92', object='fine_tuning.job.event', data={'step': 15, 'train_loss': 0.9213531613349915, 'total_steps': 300, 'train_mean_token_accuracy': 0.7799999713897705}, type='metrics'), FineTuningJobEvent(id='ftevent-e73bpgpUhjX6U9DLlAyY0FrI', created_at=1732997970, level='info', message='Step 14/300: training loss=1.28', object='fine_tuning.job.event', data={'step': 14, 'train_loss': 1.2765843868255615, 'total_steps': 300, 'train_mean_token_accuracy': 0.7272727489471436}, type='metrics'), FineTuningJobEvent(id='ftevent-HfXexsozjH4OwlhySbbTdOvv', created_at=1732997970, level='info', message='Step 13/300: training loss=0.98', object='fine_tuning.job.event', data={'step': 13, 'train_loss': 0.9794135093688965, 'total_steps': 300, 'train_mean_token_accuracy': 0.8074866533279419}, type='metrics'), FineTuningJobEvent(id='ftevent-ZuBCVnUGjv10sK6GUOdQ47dC', created_at=1732997970, level='info', message='Step 12/300: training loss=0.96', object='fine_tuning.job.event', data={'step': 12, 'train_loss': 0.9569916725158691, 'total_steps': 300, 'train_mean_token_accuracy': 0.782608687877655}, type='metrics'), FineTuningJobEvent(id='ftevent-B2qxmLvzeFMFBkBIb0DZBHYW', created_at=1732997970, level='info', message='Step 11/300: training loss=1.26', object='fine_tuning.job.event', data={'step': 11, 'train_loss': 1.2608076333999634, 'total_steps': 300, 'train_mean_token_accuracy': 0.7580645084381104}, type='metrics'), FineTuningJobEvent(id='ftevent-I3qsU2uger4yx5EMipnraIJM', created_at=1732997965, level='info', message='Step 10/300: training loss=1.15', object='fine_tuning.job.event', data={'step': 10, 'train_loss': 1.1490206718444824, 'total_steps': 300, 'train_mean_token_accuracy': 0.8034682273864746}, type='metrics'), FineTuningJobEvent(id='ftevent-uEQ7Lwc2zztRHMrY7wUvFtgj', created_at=1732997961, level='info', message='Step 9/300: training loss=1.99', object='fine_tuning.job.event', data={'step': 9, 'train_loss': 1.988956093788147, 'total_steps': 300, 'train_mean_token_accuracy': 0.6256410479545593}, type='metrics'), FineTuningJobEvent(id='ftevent-Frb54hYJKx5yQRGFbx6MmZJo', created_at=1732997961, level='info', message='Step 8/300: training loss=1.76', object='fine_tuning.job.event', data={'step': 8, 'train_loss': 1.764831304550171, 'total_steps': 300, 'train_mean_token_accuracy': 0.7479674816131592}, type='metrics'), FineTuningJobEvent(id='ftevent-HdOpd9JuwJjvT990zc1vDoRq', created_at=1732997961, level='info', message='Step 7/300: training loss=1.73', object='fine_tuning.job.event', data={'step': 7, 'train_loss': 1.729090929031372, 'total_steps': 300, 'train_mean_token_accuracy': 0.7203390002250671}, type='metrics'), FineTuningJobEvent(id='ftevent-ufWB07ylJ1EuyWIL9U9XPZq0', created_at=1732997961, level='info', message='Step 6/300: training loss=2.92', object='fine_tuning.job.event', data={'step': 6, 'train_loss': 2.9223790168762207, 'total_steps': 300, 'train_mean_token_accuracy': 0.6315789222717285}, type='metrics')], object='list', has_more=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(\"ftjob-Vod0juyldT7zqUr8CLngUUZY\")\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-Vod0juyldT7zqUr8CLngUUZY\", limit=10)\n",
    "\n",
    "\n",
    "# Cancel a job\n",
    "# client.fine_tuning.jobs.cancel(\"my_job_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_UvcHRUdnWx"
   },
   "source": [
    "You can run your fine tuned model in the OpenAI Playground.  After the model is finished finetuning you'll find it in the Model dropdown menu (you might need to press reload in your browser for your fine-tuned model to appear).\n",
    "\n",
    "## Call your fine-tuned model from the OpenAI API\n",
    "\n",
    "Alternately, you can use your fine tuned model via the API by specifying it as the model.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "sM7AvrzqdjKU"
   },
   "outputs": [],
   "source": [
    "def generate_bio(attributes, finetuned_model):\n",
    "  client = openai.OpenAI()\n",
    "  prompt = \"{attributes}\\n---\\n\".format(attributes=attributes)\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"You should generate a biography for a person based on the details provided.\"}\n",
    "  response = client.chat.completions.create(\n",
    "      model=finetuned_model,\n",
    "      messages = [\n",
    "          system_message,\n",
    "          {\"role\" : \"user\", \"content\" : prompt}\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      max_tokens=500,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"###\"]\n",
    "      )\n",
    "  return response.choices[0].message.content.strip()\n",
    "\n",
    "# Replace with your model's name\n",
    "finetuned_model = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "lMorpMvta66Y",
    "outputId": "6da66967-f9eb-4ae5-c31e-30333173def6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notable_type: computer scienist\n",
      "alma_mater: Stanford University (BS in Symbolic Systems), University of Edinburgh (PhD in Informatics)\n",
      "birth_place: California\n",
      "children: 2\n",
      "gender: male\n",
      "main_interests: Artificial Intelligence, Natural Language Processing\n",
      "name: Chris Callison-Burch\n",
      "nationality: American\n",
      "notable_works: Moses: Open source toolkit for statistical machine translation, The Paraphrase Database (PPDB)\n",
      "occupation: professor\n",
      "courses_taught: AI, Crowdsourcing and NLP\n",
      "enrollment_in_most_popular_course: 570 students\n",
      "institution: University of Pennsylvania\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chris Callison-Burch is an American computer scientist known for his work in Artificial Intelligence and Natural Language Processing. He was born in California and holds a Bachelor of Science in Symbolic Systems from Stanford University and a PhD in Informatics from the University of Edinburgh.\\n\\nChris is a professor at the University of Pennsylvania, where he teaches courses in AI, Crowdsourcing, and NLP. He is a father of two children and has made significant contributions to the field of computer science, particularly through his notable works such as Moses, an open-source toolkit for statistical machine translation, and The Paraphrase Database (PPDB).\\n\\nWith a passion for advancing the capabilities of AI and NLP, Chris has influenced many students, with his most popular course boasting an enrollment of 570 students. His dedication to research and education has solidified his reputation as a leading figure in the field of computer science.'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = \"\"\"notable_type: computer scienist\n",
    "alma_mater: Stanford University (BS in Symbolic Systems), University of Edinburgh (PhD in Informatics)\n",
    "birth_place: California\n",
    "children: 2\n",
    "gender: male\n",
    "main_interests: Artificial Intelligence, Natural Language Processing\n",
    "name: Chris Callison-Burch\n",
    "nationality: American\n",
    "notable_works: Moses: Open source toolkit for statistical machine translation, The Paraphrase Database (PPDB)\n",
    "occupation: professor\n",
    "courses_taught: AI, Crowdsourcing and NLP\n",
    "enrollment_in_most_popular_course: 570 students\n",
    "institution: University of Pennsylvania\"\"\"\n",
    "\n",
    "biography = generate_bio(attributes, finetuned_model)\n",
    "print(attributes)\n",
    "print('---')\n",
    "biography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppP6tS3FjBGa"
   },
   "source": [
    "## Analyze your model's output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ith4CGAVdGfX"
   },
   "source": [
    "Sometimes the model will add facts that are not present in the attributes.  For instance, one time it said\n",
    "> He was a member of the research staff at IBM Research in Yorktown Heights.\n",
    "\n",
    "which is not correct. Another time it said\n",
    "> His most popular course was on AI, which had 570 students.\n",
    "\n",
    "which is correct, but not specified in the attirbutes.\n",
    "\n",
    "Try running your own fine-tuned model until it produces something that wasn't licensed by the attributes.\n",
    "\n",
    "Save the good runs and the bad run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdFONhDMdWw-"
   },
   "outputs": [],
   "source": [
    "generations_with_correct_facts = [\n",
    "    \"\"\" Chris Callison-Burch is an American computer scientist known for his work in the fields of Artificial Intelligence and Natural Language Processing. \n",
    "        He was born in California and pursued his Bachelor of Science in Symbolic Systems from Stanford University, followed by a PhD in Informatics from the University of Edinburgh.\n",
    "        \\n\\nCurrently, Chris Callison-Burch serves as a professor at the University of Pennsylvania, where he teaches courses on AI, Crowdsourcing, and NLP. \n",
    "        He has made significant contributions to the field through his notable works such as Moses, an open-source toolkit for statistical machine translation, and The Paraphrase Database (PPDB).\n",
    "        \\n\\nChris is a dedicated educator with an enrollment of 570 students in his most popular course. In addition to his academic achievements, he is a father of two children. \n",
    "    \"\"\",\n",
    "    \"\"\" \n",
    "    'Chris Callison-Burch is an American computer scientist known for his work in Artificial Intelligence and Natural Language Processing. \n",
    "    He was born in California and went on to earn a BS in Symbolic Systems from Stanford University. \n",
    "    He later obtained his PhD in Informatics from the University of Edinburgh.\n",
    "    \\n\\nCurrently, Chris serves as a professor at the University of Pennsylvania, where he teaches courses on AI, Crowdsourcing, and NLP. \n",
    "    He is a notable figure in the field, with his research focusing on developing innovative tools and technologies. \n",
    "    Chris is the creator of Moses, an open-source toolkit for statistical machine translation, and The Paraphrase Database (PPDB).\n",
    "    \\n\\nWith a passion for educating the next generation of computer scientists, Chris has taught numerous students, with his most popular course enrolling 570 students. \n",
    "    Outside of his academic pursuits, Chris is a father of two children and continues to make significant contributions to the field of computer science.'\n",
    "    \"\"\",\n",
    "    ]\n",
    "\n",
    "generation_with_incorrect_facts = \"\"\"\n",
    "'Chris Callison-Burch is a renowned computer scientist originally from California, United States. \n",
    "He holds a Bachelor of Science degree in Symbolic Systems from Stanford University and a PhD in Informatics from the University of Edinburgh. \n",
    "Chris is a dedicated professor at the University of Pennsylvania, where he teaches courses on Artificial Intelligence, Crowdsourcing, and Natural Language Processing.\n",
    "\\n\\nHis main research interests lie in Artificial Intelligence and Natural Language Processing, and he has made significant contributions to the field. \n",
    "Chris is best known for developing Moses, an open-source toolkit for statistical machine translation, and for creating The Paraphrase Database (PPDB).\n",
    "\\n\\nThroughout his career, Chris has inspired and educated numerous students, with his most popular course boasting an enrollment of 570 students. \n",
    "In addition to his academic pursuits, Chris is also a loving father of two children and continues to push the boundaries of computer science with his innovative work.'\n",
    "\"\"\"\n",
    "\n",
    "# Although he does do research, this is not in the attributes\n",
    "incorrect_facts = [\n",
    "    \"His main research interests lie in Artificial Intelligence and Natural Language Processing, and he has made significant contributions to the field.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMEUR3f8eh0t"
   },
   "source": [
    "# Fine Tune a New Model\n",
    "\n",
    "Now that you've seen an example of how to do fine-tuning with the OpenAI API, let's have you write code to fine-tune your own model.\n",
    "\n",
    "For this model, I'd like you to do the reverse direction of what we just did.  Given a Wikipedia Biograph like this:\n",
    "\n",
    "> Jill Tracy Jacobs Biden (born June 3, 1951) is an American educator and the current first lady of the United States as the wife of President Joe Biden. She was the second lady of the United States from 2009 to 2017. Since 2009, Biden has been a professor of English at Northern Virginia Community College.\n",
    "\n",
    "> She has a bachelor's degree in English and a doctoral degree in education from the University of Delaware, as well as master's degrees in education and English from West Chester University and Villanova University. She taught English and reading in high schools for thirteen years and instructed adolescents with emotional disabilities at a psychiatric hospital. From 1993 to 2008, Biden was an English and writing instructor at Delaware Technical & Community College. Biden is thought to be the first wife of a vice president or president to hold a paying job during her husband's tenure.\n",
    "\n",
    "> Born in Hammonton, New Jersey, she grew up in Willow Grove, Pennsylvania. She married Joe Biden in 1977, becoming stepmother to Beau and Hunter, his two sons from his first marriage. Biden and her husband also have a daughter together, Ashley Biden, born in 1981. She is the founder of the Biden Breast Health Initiative non-profit organization, co-founder of the Book Buddies program, co-founder of the Biden Foundation, is active in Delaware Boots on the Ground, and with Michelle Obama is co-founder of Joining Forces. She has published a memoir and two children's books.\n",
    "\n",
    "Your model should output something like this:\n",
    "```\n",
    "notable_type: First Lady of the United States\n",
    "name: Jill Biden\n",
    "gender: female\n",
    "nationality: American\n",
    "birth_date: 03 June 1951\n",
    "birth_place: Hammonton, New Jersey\n",
    "alma_mater: University of Delaware\n",
    "occupation: professor of English at Northern Virginia Community College\n",
    "notable_works: children's books and memoir\n",
    "main_interests: education, literacy, women's health\n",
    "partner: Joe Biden\n",
    "children: Ashley Biden, Beau Biden (stepson), Hunter Biden (stepson)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "id": "fnR7bToueV50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"messages\": [{\"role\": \"system\", \"content\": \"Analyse the inputed biography and output the \"}, {\"role\": \"user\", \"content\": \"Biography: Liza Tugel (20 May 1917 - 25 July 1941) was a Kyrgyzstani horse racer. She was born in Bosteri, Kyrgyzstan. Her active years are from 1925 to 1941. She died of tuberculosis on 25 July 1941 in Kyrgyzstan and was buried in S\\u00f6ktsk Gorge. She was born to Yekaterina Sergeyevna Tugel and Kazybek Moldagalloevich Tugel. She attended primary school. Her height was 5ft 1in and weight was 110lb. She married her partner Anastasia Iosifovna Tugel and had one child Leyla.\\n###\"}, {\"role\": \"assistant\", \"content\": \"notable_type: athlete\\nname: Liza Tugel\\ngender: female\\nnationality: Kyrgyzstani\\nbirth_date: 20 May 1917\\nbirth_place: Bosteri, Kyrgyzstan\\ndeath_date: 25 July 1941\\ndeath_place: Kyrgyzstan\\ndeath_cause: tuberculosis\\nresting_place: S\\u00f6ktsk Gorge\\nsport: horse racing\\ncountry: Kyrgyzstan\\ncitizenship: none\\neducation: primary school\\nevent: horse racing\\nposition: none\\nyears_active: 1925-1941\\nheight: 5ft 1in\\nweight: 110lb\\nmother: Yekaterina Sergeyevna Tugel\\nfather: Kazybek Moldagalloevich Tugel\\npartner: Anastasia Iosifovna Tugel\\nchildren: Leyla\\n---\\n\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"Analyse the inputed biography and output the \"}, {\"role\": \"user\", \"content\": \"Biography: Koba Tsiklauri (born May 18, 1989) is a Georgian figure skater. Tsiklauri\\u00b4s hometown is Moscow, Russia. They completed education in Moscow Institute of Physics and Technology. Retired in the year 2019. Tsiklauri\\u00b4s coach is Vladimir Tsiklauri. They was born to Tamara Koba and Mikhail Koba. They are in a relationship with Tatiana Proskurina.\\n###\"}, {\"role\": \"assistant\", \"content\": \"notable_type: athlete\\nname: Koba Tsiklauri\\ngender: non-binary\\nnationality: Georgia\\nbirth_date: 18 May 1989\\nbirth_place: Tbilisi, Georgia\\nsport: figure skating\\ncountry: Russia\\nhometown: Moscow, Russia\\neducation: Moscow Institute of Physics and Technology\\nevent: figure skating\\nposition: singles\\nyears_active: 1989-2019\\nretired: 2019\\nheight: 5ft 3in\\nweight: 180lb\\ncoach: Vladimir Tsiklauri\\nmother: Tamara Koba\\nfather: Mikhail Koba\\npartner: Tatiana Proskurina\\n---\\n\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"Analyse the inputed biography and output the \"}, {\"role\": \"user\", \"content\": \"Biography: Amjad Al Ansari (born 19 June 1952) is a Qatari judoka. He competed in the men's lightweight event at the 1984 Summer Olympics. Their parents were Saima Al Nasser and Abu Hashim Al Nasser.\\n###\"}, {\"role\": \"assistant\", \"content\": \"notable_type: athlete\\nname: Amjad Al-Ansari\\ngender: non-binary\\nnationality: Qatari\\nbirth_date: 19 June 1952\\nbirth_place: Abu Dhabi\\nsport: judo\\ncountry: Qatar\\nhometown: Doha\\ncitizenship: Qatari\\nevent: judo\\nyears_active: 1978-1990\\nretired: 1990\\nheight: 6ft 5in\\nweight: 218lb\\ncoach: Ali Al Saadi\\nnational_team: Qatar\\nworlds: 1984, 1988\\nolympics: 1984\\nmother: Saima Al Nasser\\nfather: Abu Hashim Al Nasser\\nchildren: Hashim and Hamad\\n---\\n\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"Analyse the inputed biography and output the \"}, {\"role\": \"user\", \"content\": \"Biography: Joshua Odongo was a Kenyan intelligence operator who worked for the United States and Ethiopian militaries. Odongo was born on May 25, 1940 in Nairobi, Kenya to Michael Odongo and Carol Odongo. He attended the University of Nairobi and later became a civil service employee at the Ministry of Agriculture. Odongo began his career as an intelligence operator in 1971 and remained in the field until his retirement in 2008. He was known as \\\"Eagle\\\" to his colleagues and was credited with playing a major role in helping the US and Ethiopian militaries identify the location of the main camps of terrorist groups in Kenya. Odongo died in 2008. He is the agent for Department of Defense. His operation in Ethiopia called Operation Thunderhawk. Joshua Odongo married Janet Amimo.\\n###\"}, {\"role\": \"assistant\", \"content\": \"notable_type: spy\\nname: Joshua Odongo\\ngender: male\\nnationality: Kenyan\\nbirth_date: 25 May 1940\\nbirth_place: Nairobi, Kenya\\nserviceyears: 1971-2008\\nknown_for: intelligence operator that helped to identify the location of the main camps of terrorist groups in Kenya for the US and Ethiopian militaries\\nalma_mater: University of Nairobi\\noccupation: civil service employee at the Ministry of Agriculture\\ncodename: Eagle\\nallegiance: United States of America\\nagency: Department of Defense\\noperation: Operation Thunderhawk\\nmother: Carol Odongo\\nfather: Michael Odongo\\npartner: Janet Amimo\\n---\\n\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"Analyse the inputed biography and output the \"}, {\"role\": \"user\", \"content\": \"Biography: Almanzo Lund was born on April 30, 1961 in Geneva, Switzerland. He is the son of Louis and Carole Lund. Lund attended Art Center College of Design, where he earned a BA in illustration. Lund is also an artist, curator, and owner of the art gallery Lund Galerie in Bern, Switzerland. Lund is also a painter, sculptor, and photographer. Lund is also the father of Jeanne Lund and his partner is Joan Lund. He attended Art Center College of Design, where he pursued a degree in art. Lund was known for his works The Blue Hour (2017), The Red Hour (2016), The Orange Hour (2015), and The Green Hour (2014). He died on August 17, 2016 after suffering from a brain aneurysm and buried in Muralto, Switzerland.\\n###\"}, {\"role\": \"assistant\", \"content\": \"notable_type: artist\\nname: Almanzo Lund\\ngender: male\\nnationality: Swiss\\nbirth_date: 30 April 1961\\nbirth_place: Geneva, Switzerland.\\ndeath_date: August 17, 2016\\ndeath_place: Bern, Switzerland.\\ndeath_cause: brain aneurysm\\nresting_place: Muralto, Switzerland\\nknown_for: art gallery, art curation, artist, painter, sculptor\\nnotable_works: The Blue Hour (2017), The Red Hour (2016), The Orange Hour (2015)\\nmovement: expressionism\\nalma_mater: Art Center College of Design.\\nawards: two purple hearts.\\nelected: curator\\nmother: Carole Lund\\nfather: Louis Lund\\npartner: Joan Lund\\nchildren: Jeanne Lund\\n---\\n\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"Analyse the inputed biography and output the \"}, {\"role\": \"user\", \"content\": \"Biography: Vadzii Gudavich (born on 28 September 1888 in Riga, Latvia - died on 14 March 1985 in Riga, Latvia) was a Latvian who made many ascents including Mt. Everest, Mt. Lhotse, Mt. Manaslu, Mt. Annapurna, K2, Cerro Fitz Roy, Patagonia's 3 Towers. His start age was 18. His father was Janis Gudavich and his mother was Anna Gudavich. He had partner named Nikifor Akulovs and had two children: Arturs Gudavich and Inara Gudavich. He died of old age and was buried in Riga Forest Cemetary.\\n###\"}, {\"role\": \"assistant\", \"content\": \"notable_type: mountaineer\\nname: Vadzii Gudavich\\ngender: male\\nnationality: Latvian\\nbirth_date: 28 September 1888\\nbirth_place: Riga, Latvia\\ndeath_date: March 14, 1985\\ndeath_place: Riga, Latvia\\ndeath_cause: Old Age\\nresting_place: Riga Forest Cemetary\\nstart_age: 18\\nnotable_ascents: Mt. Everest, Mt. Lhotse, Mt. Manaslu, Mt. Annapurna, K2, Cerro Fitz Roy, Patagonia's 3 Towers\\nmother: Anna Gudavich\\nfather: Janis Gudavich\\npartner: Nikifor Akulovs\\nchildren: Arturs Gudavich, Inara Gudavich\\n---\\n\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"Analyse the inputed biography and output the \"}, {\"role\": \"user\", \"content\": \"Biography: Naomi Aba (born in 1992 in Riga, Latvia) is a non-binary Latvian mountaineer born to Ameena Aba and Mohan Aba. Aba began their mountaineering career at the age of 13. Aba notable ascents were the Grand Canyon, K2, Denali and thier final ascent was Mt. Everest. Aba had partnerships with Saskia Kapoor and Nisha Rathore. Aba was married to Saskia Kapoor and together had Arjun as their child.\\n###\"}, {\"role\": \"assistant\", \"content\": \"notable_type: mountaineer\\nname: Naomi Aba\\ngender: non-binary\\nnationality: Latvian\\nbirth_date: Riga, Latvia, 1992\\nstart_age: 13\\nnotable_ascents: the Grand Canyon, K2, Denali\\nfinal_ascent: Mt. Everest\\npartnerships: Saskia Kapoor and Nisha Rathore\\nmother: Ameena Aba\\nfather: Mohan Aba\\npartner: Saskia Kapoor\\nchildren: Arjun\\n---\\n\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"Analyse the inputed biography and output the \"}, {\"role\": \"user\", \"content\": \"Biography: Juan Perez was a Costa Rican writer. He was born in San Jose, Costa Rica on 06 January 1942. Juan was educated at the University of Costa Rica and completed his Bachelors in literature, and Doctorate in linguistics. Catalina Perez and Miguel Garcia were Juan Perez's parents. His language was Spanish. Rafael Perez was Juan Perez's child. Perez died in San Jose on May 12, 2010.\\n###\"}, {\"role\": \"assistant\", \"content\": \"notable_type: writer\\nname: Juan Perez\\ngender: male\\nnationality: Costa Rican\\nbirth_date: 06 January 1942\\nbirth_place: Costa Rica\\ndeath_date: 12 May 2010\\ndeath_place: San Jose, Costa Rica\\nalma_mater: University of Costa Rica\\neducation: Bachelors in literature, Doctorate in linguistics\\noccupation: writer\\nlanguage: Spanish\\nmother: Catalina Perez\\nfather: Miguel Garcia\\nchildren: Rafael Perez\\n---\\n\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"Analyse the inputed biography and output the \"}, {\"role\": \"user\", \"content\": \"Biography: Saran Sig (aka The Green Witch) is a Greenlandic guitarist. Sig was born in 1930 in Qasigiannguit, Greenland. They have been active since 1970 to 2021. Their associated acts were Etikett folket, Arctic Animals, and The Moon. Their mother is Naja Quseeraq. Sig received most influential of the 1970s. They are a self taught person. Their label is Etikett folket. Their genre is psychedelic and folk.\\n###\"}, {\"role\": \"assistant\", \"content\": \"notable_type: musician\\nname: Saran Sig\\nbirth_name: Vilhelm\\nalias: The Green Witch\\ngender: non-binary\\nbirth_date: 17 December 1930\\nbirth_place: Qasigiannguit, East Greenland\\ninstrument: guitar\\ngenre: psychedelic, folk\\nhometown: Qasigiannguit\\nnationality: Greenlandic\\ncitizenship: Greenlandic\\neducation: self-taught\\nyears_active: 1970-2021\\nlabel: Etikett folket\\nassociated_acts: Etikett folket, Arctic Animals, The Moon, etc.\\nawards: Most Influential of the '70s\\nmother: Naja Quseeraq\\n---\\n\"}]}\n",
      "{\"messages\": [{\"role\": \"system\", \"content\": \"Analyse the inputed biography and output the \"}, {\"role\": \"user\", \"content\": \"Biography: Sara Th\\u00e6rskel (aka Sara Pedersen) was born on July 21, 1905. Her parents were Malena Th\\u00e6rskel and Tholver Th\\u00e6rskel. She went to Unfinished high school (1920); Midwifery at the Greenwich Nurses' College (1923); Completing high school (1932). Sara received Male Vocalist of the Year (1954), \\u00d3skar Award (1957, 1960), Composer of the Year by T\\u00f3nlistear\\u00e1s (1957), Composer of the Year by T\\u00f3nlist (1972, 1974). She passed away on January 5, 1987 in Reykjav\\u00edk, Iceland due to heart attack and remains were done in Vesturb\\u00e6r Cemetery.\\n###\"}, {\"role\": \"assistant\", \"content\": \"notable_type: musician\\nname: Sara Pedersen\\nbirth_name: Sara Olsen\\nalias: Sara Pedersen, Sara Th\\u00e6rskel, \\u00c1nana, \\u00c1nana Th\\u00e6rskel\\ngender: female\\nbirth_date: 21 July 1905\\ndeath_place: Reykjav\\u00edk, Iceland\\ndeath_cause: heart attack\\nresting_place: Vesturb\\u00e6r Cemetery\\ninstrument: piano, violin\\ngenre: Nordic folk\\nhometown: Aasiaat, Greenland\\nnationality: Greenlandic\\ncitizenship: Danish\\neducation: Unfinished high school (1920); Midwifery at the Greenwich Nurses' College (1923); Completing high school (1932)\\nyears_active: 1952-1987\\nlabel: Geysir\\nassociated_acts: \\u00c1nana\\nawards: Male Vocalist of the Year (1954), \\u00d3skar Award (1957, 1960), Composer of the Year by T\\u00f3nlistear\\u00e1s (1957), Composer of the Year by T\\u00f3nlist (1972, 1974)\\nmother: Malena Th\\u00e6rskel\\nfather: Tholver Th\\u00e6rskel\\npartner: Gu\\u00f0mundur H. Gu\\u00f0mundsson\\nchildren: Brynj\\u00f3lfur Gu\\u00f0mundsson, Gu\\u00f0mundur Gu\\u00f0mundsson, Th\\u00e6rskel Th\\u00e6rskelsson\\n---\\n\"}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def create_wikibio_parser_finetuning_data(wikibios, fine_tuning_filename):\n",
    "  # TODO - write your fine-tuning function\n",
    "  fine_tuning_data = []\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"Analyse the inputed biography and output the \"}\n",
    "\n",
    "  for attributes, bio in wikibios:\n",
    "    completion = \"{attributes}\\n---\\n\".format(attributes=attributes)\n",
    "    prompt = \"Biography: {bio}\\n###\".format(bio=bio)\n",
    "    data = {\"messages\" : [system_message, {\"role\" : \"user\", \"content\" : prompt}, {\"role\" : \"assistant\", \"content\" : completion}]}\n",
    "    fine_tuning_data.append(data)\n",
    "\n",
    "  random.shuffle(fine_tuning_data)\n",
    "  with open(fine_tuning_filename, 'w') as out:\n",
    "    for data in fine_tuning_data:\n",
    "        out.write(json.dumps(data))\n",
    "        out.write('\\n')\n",
    "\n",
    "fine_tuning_filename='wikibio_parser_finetuning_data.jsonl'\n",
    "create_wikibio_parser_finetuning_data(wiki_bios, fine_tuning_filename)\n",
    "!head '{fine_tuning_filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "id": "apOQjZ7miIdL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-E4frVQmmLqxViko6Vg6QxT', bytes=146405, created_at=1733023431, filename='wikibio_parser_finetuning_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the file to OpenAI's API (only do this when you're confident your data looks right.)\n",
    "client.files.create(\n",
    "  file=open(\"wikibio_parser_finetuning_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "id": "kzUEaol5eoPG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-LZKJNhsEytVJrSXJ2OOvlpew', created_at=1733023451, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-CtG7bJ67coneBV3uvISitdeu', result_files=[], seed=68150432, status='validating_files', trained_tokens=None, training_file='file-E4frVQmmLqxViko6Vg6QxT', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a fine-tune job. Careful! ($$$)\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-E4frVQmmLqxViko6Vg6QxT\",\n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrqNIFPFmtEQ"
   },
   "outputs": [],
   "source": [
    "def parse_bio(biography, finetuned_bio_parser_model):\n",
    "  # TODO call the API with your fine-tuned model, return a string representing the attributes\n",
    "  client = openai.OpenAI()\n",
    "  prompt = \"Biography: {biography}\\n###\".format(biography=biography)\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"In one string, produce thirty common attributes for the person with the information given.\"}\n",
    "  response = client.chat.completions.create(\n",
    "      model=finetuned_bio_parser_model,\n",
    "      messages = [\n",
    "          system_message,\n",
    "          {\"role\" : \"user\", \"content\" : prompt}\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      max_tokens=500,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"###\"]\n",
    "      )\n",
    "  return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "finetuned_bio_parser_model=\"ft:gpt-3.5-turbo-0125:personal::AZV0fhgm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender: male\n",
      "nationality: Congolese\n",
      "birth_date: 10 October 1919\n",
      "birth_place: Léopoldville, Belgian Congo\n",
      "death_date: 31 October 2012\n",
      "death_place: Paris, France\n",
      "death_cause: pancreatic cancer\n",
      "resting_place: Boissy-Saint-Léger\n",
      "alma_mater: Louvain Catholic University, Catholic University of Leuven, University of Leuven, Catholic University of Belgium, Catholic University of Paris\n",
      "occupation: theologian, catholic priest\n",
      "notable_works: Theology of God\n",
      "main_interest: Theology of God\n",
      "mother: Mwadibu\n",
      "father: Agathe Ngalaka\n",
      "partner: Odette Ngalaka\n",
      "children: one child\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Delete this at the end, this is just to test my tuner \n",
    "# bio = \"Igor Ivanov was born on October 18, 1970 in Moscow, Russia. His mother is Anastasia Nikolaevna Ivanova, and his father is Nikolay Ivanovich Ivanov. Ivanov is a Russian artist who specializes in oil paintings of portraits, city scenes, still lifes, and flowers. Ivanov attended the Moscow College of Art (now in the Moscow Institute of Painting, Sculpture and Architecture), where he received a bachelor's degree in painting and a master's degree in art history. His notable works include Portraits of Celebrities, Still Life with Flowers, Flowers and Fruit and movement realism. He has received several awards, including the People's Artist of the USSR, Hero of Socialist Labour. Ivanov is also a Professor of the Academy, and he was elected a People's Artist of Russia in 2006. He is married to Alexandra Alexandrovna Vasilieva, and they have two daughters, Alexandra Ivanovna Vasilieva and Anastasia Ivanovna Vasilieva.\"\n",
    "bio = \"Ansu Ngalaka (10 October 1919 - 31 October 2012) was a Congolese theologian and catholic priest. He was born in Léopoldville, Belgian Congo to a wealthy Christian family. He attended the Louvain Catholic University, Catholic University of Leuven, University of Leuven, Catholic University of Belgium, Catholic University of Paris and his main interest was in Theology of God. He was born to Mwadibu, Agathe Ngalaka and was married to Odette Ngalaka. Ansu was Catholic and he had one child. Anys died due to pancreatic cancer in Paris, France and is buried in Boissy-Saint-Léger.\"\n",
    "\n",
    "test_attributes = parse_bio(bio, finetuned_bio_parser_model)\n",
    "print(test_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9V3zLFmqEbq"
   },
   "source": [
    "## Test your parser\n",
    "\n",
    "Next we will test your parser.  This will involve calling your `parse_bio` function about 250 times, so be sure that you've got it properly debugged and working before running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "id": "v9a9CvE9p8D_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  649k  100  649k    0     0   805k      0 --:--:-- --:--:-- --:--:--  805k\n"
     ]
    }
   ],
   "source": [
    "!curl -o SynthBio_test.json https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "id": "Ncu11s25qdoV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_wiki_bio_test_set(filename='SynthBio_test.json', max_test_items=10, randomized=True):\n",
    "  \"\"\"\n",
    "  Loads our wikibio test set, and returns a list of tuples\n",
    "  biographies (text), attributes (dictionaires)\n",
    "  \"\"\"\n",
    "  with open(filename) as f:\n",
    "    synth_bio_data = json.load(f)\n",
    "  bios = []\n",
    "  for data in synth_bio_data:\n",
    "    notable_type = data['notable_type']\n",
    "    attributes = data['attrs']\n",
    "    attributes['notable_type'] = notable_type\n",
    "    biography = data['biographies'][0]\n",
    "    bios.append((biography, attributes))\n",
    "  return bios[:min(max_test_items, len(bios))]\n",
    "\n",
    "def convert_to_dict(predcited_attributes_txt):\n",
    "  \"\"\"\n",
    "  Converts predicted attributes from text format into a dictionary.\n",
    "  \"\"\"\n",
    "  predicted_attributes = {}\n",
    "  for line in predcited_attributes_txt.split('\\n'):\n",
    "    try:\n",
    "      attribute, value = line.split(':')\n",
    "      predicted_attributes[attribute.strip()] = value.strip()\n",
    "    except ValueError:\n",
    "      continue\n",
    "  return predicted_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giP9b76iFjEj"
   },
   "source": [
    "Helper function for computing precision, recall and f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "id": "8PvGbJYKrEq7"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def update_counts(gold_attributes, predicted_attributes, true_positives, false_positives, false_negatives, all_attributes):\n",
    "  # Compute true positives and false negatives\n",
    "  for attribute in gold_attributes:\n",
    "    all_attributes[attribute] += 1\n",
    "    if attribute in predicted_attributes:\n",
    "      # gold attribute exists in predicted attributes\n",
    "      # some attributes have multiple values\n",
    "      predicted_values = predicted_attributes[attribute].split(',')\n",
    "      gold_values = gold_attributes[attribute].split(',')\n",
    "      for value in gold_values:\n",
    "        if value.strip() in predicted_values:\n",
    "          # gold attribute exists in predicted attributes and gold value exists in predicted values\n",
    "          true_positives[attribute] += 1\n",
    "        else:\n",
    "          # gold attribute exists in predicted attributes but gold value does not exist in predicted values\n",
    "          false_negatives[attribute] += 1\n",
    "    else:\n",
    "      # gold attribute does not exist in predicted attributes\n",
    "      false_negatives[attribute] += 1\n",
    "\n",
    "  # Compute false positives\n",
    "  for attribute in predicted_attributes:\n",
    "    if attribute not in gold_attributes:\n",
    "      # predicted attribute does not exist in gold attributes\n",
    "      all_attributes[attribute] += 1\n",
    "      false_positives[attribute] += 1\n",
    "    else:\n",
    "      # predicted attribute exists in gold attributes\n",
    "      # some attributes have multiple values\n",
    "      gold_values = gold_attributes[attribute].split(',')\n",
    "      predicted_values = predicted_attributes[attribute].split(',')\n",
    "      for value in predicted_values:\n",
    "        if value.strip() not in gold_values:\n",
    "          # predicted attribute exists in gold attributes but predicted value does not exist in gold values\n",
    "          false_positives[attribute] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "id": "M5UXVTgaGHBD"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_on_test_set(finetuned_bio_parser_model, wiki_bio_test, threshold_count = 5):\n",
    "  \"\"\"\n",
    "  Computer the precision, recall and f-score for each of the attributes\n",
    "  that appears more than the treshold count\n",
    "  \"\"\"\n",
    "  true_positives = Counter()\n",
    "  false_positives = Counter()\n",
    "  false_negatives = Counter()\n",
    "  all_attributes = Counter()\n",
    "\n",
    "  for bio, gold_attributes in wiki_bio_test:\n",
    "    predicted_attributes = convert_to_dict(parse_bio(bio, finetuned_bio_parser_model))\n",
    "    update_counts(gold_attributes, predicted_attributes, true_positives, false_positives, false_negatives, all_attributes)\n",
    "\n",
    "  average_precision = 0\n",
    "  average_recall = 0\n",
    "  total = 0\n",
    "\n",
    "  for attribute in all_attributes:\n",
    "    if all_attributes[attribute] < threshold_count:\n",
    "      continue\n",
    "    print(attribute.upper())\n",
    "    try:\n",
    "      precision = true_positives[attribute] / (true_positives[attribute] + false_positives[attribute])\n",
    "    except:\n",
    "      precision = 0.0\n",
    "    try:\n",
    "      recall = true_positives[attribute] / (true_positives[attribute] + false_negatives[attribute])\n",
    "    except:\n",
    "      recall = 0.0\n",
    "    print(\"precision:\", precision)\n",
    "    print(\"recall:\", recall)\n",
    "    print(\"f-score:\", (precision+recall)/2)\n",
    "    print('---')\n",
    "    average_precision += precision\n",
    "    average_recall += recall\n",
    "    total += 1\n",
    "\n",
    "  print(\"AVERAGE\")\n",
    "  average_precision = average_precision/total\n",
    "  average_recall = average_recall/total\n",
    "  print(\"precision:\", average_precision)\n",
    "  print(\"recall:\", average_recall)\n",
    "  print(\"f-score:\", (average_precision+average_recall)/2)\n",
    "  print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBCI95rQI8gk"
   },
   "source": [
    "If you would like to evaluate on the full test set, there are 237 test items.  You can set `max_test_items=237`.  Doing so will call your `parse_bio` function about 237 times, so be sure that you've got it properly debugged and working before running this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AuQMn2Oky59"
   },
   "source": [
    "# **Important: Grading Criteria**\n",
    "\n",
    "For a thorough evaluation of the model, max_test_items has to be set to a **minimum of 50**. To receive full credit, you have to submit a previously run notebook with all outputs included. Full credit will be awarded if **all three metrics—average precision, average recall, and average f-score—exceed 0.4 (40%) on at least 50 examples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Igor Ivanov', 'gender': 'male', 'nationality': 'Russian', 'birth_date': '18 October 1970', 'birth_place': 'Moscow, Russian Soviet Federative Socialist Republic', 'known_for': 'paintings of portraits, city scenes, still lifes, and flowers', 'notable_works': 'Portraits of Celebrities, Still Life with Flowers, Flowers and Fruit', 'movement': 'realism', 'alma_mater': 'the Moscow College of Art (now in the Moscow Institute of Painting, Sculpture and Architecture)', 'awards': \"People's Artist of the USSR, Gold Medals, Hero of Socialist Labour\", 'elected': \"People's Artist of Russia, Professor of the Academy\", 'mother': 'Anastasia Nikolaevna Ivanova', 'father': 'Nikolay Ivanovich Ivanov', 'partner': 'Alexandra Alexandrovna Vasilieva', 'children': 'Alexandra Ivanovna Vasilieva, Anastasia Ivanovna Vasilieva', 'notable_type': 'artist'}\n",
      "gold_attributes:^^^^^^^^^^^^^^^^^^^^^^\n",
      "Igor Ivanov was born on October 18, 1970 in Moscow, Russia. His mother is Anastasia Nikolaevna Ivanova, and his father is Nikolay Ivanovich Ivanov. Ivanov is a Russian artist who specializes in oil paintings of portraits, city scenes, still lifes, and flowers. Ivanov attended the Moscow College of Art (now in the Moscow Institute of Painting, Sculpture and Architecture), where he received a bachelor's degree in painting and a master's degree in art history. His notable works include Portraits of Celebrities, Still Life with Flowers, Flowers and Fruit and movement realism. He has received several awards, including the People's Artist of the USSR, Hero of Socialist Labour. Ivanov is also a Professor of the Academy, and he was elected a People's Artist of Russia in 2006. He is married to Alexandra Alexandrovna Vasilieva, and they have two daughters, Alexandra Ivanovna Vasilieva and Anastasia Ivanovna Vasilieva.\n",
      "{'\"name\"': '\"Igor Ivanov\",', '\"gender\"': '\"male\",', '\"nationality\"': '\"Russian\",', '\"birth_date\"': '\"18 October 1970\",', '\"birth_place\"': '\"Moscow, Russia\",', '\"alma_mater\"': '\"Moscow College of Art\",', '\"education\"': '\"Bachelor\\'s degree in painting and a master\\'s degree in art history\",', '\"occupation\"': '\"artist\",', '\"notable_works\"': '\"Portraits of Celebrities, Still Life with Flowers, Flowers and Fruit and movement realism\",', '\"style\"': '\"realism\",', '\"awards\"': '\"People\\'s Artist of the USSR, Hero of Socialist Labour\",', '\"elected\"': '\"People\\'s Artist of Russia in 2006\",', '\"mother\"': '\"Anastasia Nikolaevna Ivanova\",', '\"father\"': '\"Nikolay Ivanovich Ivanov\",', '\"partner\"': '\"Alexandra Alexandrovna Vasilieva\",', '\"children\"': '\"two daughters, Alexandra Ivanovna Vasilieva and Anastasia Ivanovna Vasilieva\",', '\"hometown\"': '\"Moscow, Russia\",', '\"language\"': '\"Russian\"'}\n",
      "predicted attributes:^^^^^^^^^^^^^^^^^\n",
      "{'name': 'Ansu Ngalaka', 'gender': 'male', 'nationality': 'Congolese', 'birth_date': '10 October 1919', 'birth_place': 'Léopoldville, Belgian Congo', 'death_date': '31 October 2012', 'death_place': 'Paris, France', 'death_cause': 'pancreatic cancer', 'resting_place': 'Boissy-Saint-Léger', 'alma_mater': 'Louvain Catholic University, Catholic University of Leuven, University of Leuven, Catholic University of Belgium, Catholic University of Paris', 'occupation': 'professor of theology, cardinal', 'tradition_movement': 'Catholicism', 'main_interests': 'Theology of God', 'mother': 'Agathe Ngalaka', 'father': 'Mwadibu', 'partner': 'Odette Ngalaka', 'children': 'François Ngalaka', 'notable_type': 'theologian'}\n",
      "gold_attributes:^^^^^^^^^^^^^^^^^^^^^^\n",
      "Ansu Ngalaka (10 October 1919 - 31 October 2012) was a Congolese theologian and catholic priest. He was born in Léopoldville, Belgian Congo to a wealthy Christian family. He attended the Louvain Catholic University, Catholic University of Leuven, University of Leuven, Catholic University of Belgium, Catholic University of Paris and his main interest was in Theology of God. He was born to Mwadibu, Agathe Ngalaka and was married to Odette Ngalaka. Ansu was Catholic and he had one child. Anys died due to pancreatic cancer in Paris, France and is buried in Boissy-Saint-Léger.\n",
      "{'\"name\"': '\"Ansu Ngalaka\",', '\"gender\"': '\"male\",', '\"nationality\"': '\"Congolese\"', '\"birth_date\"': '\"10 October 1919\",', '\"birth_place\"': '\"Léopoldville, Belgian Congo\",', '\"death_date\"': '\"31 October 2012\",', '\"death_place\"': '\"Paris, France\",', '\"death_cause\"': '\"pancreatic cancer\",', '\"resting_place\"': '\"Boissy-Saint-Léger\",', '\"alma_mater\"': '\"Louvain Catholic University, Catholic University of Leuven, University of Leuven, Catholic University of Belgium, Catholic University of Paris\",', '\"occupation\"': '\"theologian, catholic priest\",', '\"main_interests\"': '\"Theology of God\",', '\"mother\"': '\"Agathe Ngalaka\",', '\"father\"': '\"Mwadibu\",', '\"partner\"': '\"Odette Ngalaka\",', '\"children\"': '\"one child\",', '\"notable_works\"': '\"Léopoldville, Belgian Congo\",', '\"awards\"': '\"none\",', '\"tradition_movement\"': '\"Catholic\",', '\"hometown\"': '\"Léopoldville, Belgian Congo\",', '\"language\"': '\"French\",'}\n",
      "predicted attributes:^^^^^^^^^^^^^^^^^\n",
      "{'name': 'Choi Seung-Ho', 'gender': 'male', 'nationality': 'South Korean', 'birth_date': '01 March 1977', 'birth_place': 'South Korea', 'serviceyears': '1998-Present', 'known_for': 'leader of South Korean counterintelligence team known as the \"Bluehouse Nine\"', 'alma_mater': 'South Korean Military Academy', 'occupation': 'South Korean Army Major', 'codename': '\"Tiger Shark\"', 'allegiance': 'South Korea', 'agency': 'South Korea Army', 'mother': 'Kwang-Suk Joo', 'father': 'N/a', 'partner': 'Lee Jung-Min', 'children': 'Lee Mi-Jin, Jee Il, Jee Yoon', 'notable_type': 'spy'}\n",
      "gold_attributes:^^^^^^^^^^^^^^^^^^^^^^\n",
      "Choi Seung-Ho is a South Korean major and leader of the South Korean counterintelligence team known as the \"Bluehouse Nine\". He is born on March 1, 1977 in South Korea to Kwang-Suk Joo. He began his service in 1998. Seung went to South Korean Military Academy. He is known as \"Tiger Shark\". Seung has a partner named Lee Jung-Min and two children named Lee Mi-Jin and Jee Il.\n",
      "{}\n",
      "predicted attributes:^^^^^^^^^^^^^^^^^\n",
      "{'name': 'Alexandra Oriola', 'birth_name': 'Luisa Soto Cabeza', 'alias': 'Luso-Dominican', 'gender': 'non-binary', 'birth_date': '26 February 1896', 'birth_place': 'Santo Domingo, Dominican', 'instrument': 'violin, guitars', 'genre': 'pop-rock, alt-rock, world fusion, rockabilly', 'hometown': 'Toronto, Canada', 'nationality': 'Dominican', 'citizenship': 'Canadian', 'education': 'high school, and a B.Mus in violin and a BA in psychology', 'years_active': '1996-present', 'label': 'independent', 'associated_acts': 'Accordions Anonymous', 'awards': 'Grammy Award for violin (2019)', 'mother': 'Doña Maria Soto', 'father': 'Domingo Oriola', 'children': 'Dominique and Joséphine', 'notable_type': 'musician'}\n",
      "gold_attributes:^^^^^^^^^^^^^^^^^^^^^^\n",
      "Luso-Dominican Alias Alexandra Oriola is born on 26 February 1896 in Santo Domingo, Dominican Republic to Doña Maria Soto and Luisa Soto Cabeza and Domingo Oriola . Oriola plays violin and guitars. Their education was in high school, and a B.Mus in violin and a BA in psychology. Oriola associated acts were Accordions Anonymous. They received Grammy Award for violin (2019). Oriola have two children, Dominique and Joséphine.\n",
      "{'name': 'Alexandra Oriola', 'gender': 'non-binary', 'nationality': 'Luso-Dominican', 'birth_date': '26 February 1896', 'birth_place': 'Santo Domingo, Dominican Republic', 'instrument': 'violin and guitars', 'genre': 'indie folk', 'hometown': 'Santo Domingo, Dominican Republic', 'education': 'high school, a B.Mus in violin and a BA in psychology', 'awards': 'Grammy Award for violin (2019)', 'associated_acts': 'Accordions Anonymous', 'mother': 'Doña Maria Soto', 'father': 'Luisa Soto Cabeza and Domingo Oriola', 'children': 'Dominique and Joséphine'}\n",
      "predicted attributes:^^^^^^^^^^^^^^^^^\n",
      "{'name': 'Sefa Amanu', 'gender': 'male', 'birth_date': '28 April 1953', 'birth_place': 'Samoa', 'death_date': '28 April 1975', 'instrument': 'vocals', 'genre': 'psychedelic rock', 'hometown': 'Samoa', 'nationality': 'Samoan', 'citizenship': 'American', 'education': 'None', 'years_active': '1970-present', 'label': 'Samoan Records (Samoa)', 'associated_acts': \"Aphrodite's Child, Rembrandt Effluvia\", 'awards': 'Platinum disc (Rembrandt Effluvia, 2010)', 'mother': 'Fiasaga Amanu', 'father': 'Leaupepetele Amanu', 'children': 'None', 'notable_type': 'musician'}\n",
      "gold_attributes:^^^^^^^^^^^^^^^^^^^^^^\n",
      "Sefa Amanu born on 28 April 1975. His genre on psychedelic rock. Rembrandt Effluvia, who are based in Los Angeles, were scheduled to perform in the United States later this month. The album was critically acclaimed, and won a platinum disc. Rembrandt Effluvia released their second album \"Samoa II\" in 2011. It was also a critical success, and won a platinum disc. He was present on 1970-present. Sefa parents rae Leaupepetele Amanu,Fiasaga Amanu.\n",
      "{'\"name\"': '\"Sefa Amanu\",', '\"gender\"': '\"male\",', '\"nationality\"': '\"Samoan\",', '\"birth_date\"': '\"28 April 1975\",', '\"birth_place\"': '\"Afenga, Samoa\",', '\"instrument\"': '\"vocals\",', '\"genre\"': '\"psychedelic rock\",', '\"hometown\"': '\"Los Angeles\",', '\"citizenship\"': '\"Samoan\",', '\"education\"': '\"Dunlap Valley Middle School\",', '\"years_active\"': '\"1970-present\",', '\"label\"': '\"Rembrandt Effluvia\",', '\"associated_acts\"': '\"Rembrandt Effluvia\",', '\"notable_instruments\"': '\"Tamborine\",', '\"mother\"': '\"Leaupepetele Amanu\",', '\"father\"': '\"Fiasaga Amanu\",', '\"partner\"': '\"none\",', '\"children\"': '\"none\",', '\"awards\"': '\"none\"'}\n",
      "predicted attributes:^^^^^^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "# Delete this later\n",
    "testset_filename = 'SynthBio_test.json'\n",
    "max_test_items = 5  # set to 10 for initial testing; for final testing and submission, choose a value between 50 and 237\n",
    "wiki_bio_test = load_wiki_bio_test_set(testset_filename, max_test_items)\n",
    "# print(wiki_bio_test)\n",
    "for bio, gold_attributes in wiki_bio_test:\n",
    "    print(gold_attributes)\n",
    "    print(\"gold_attributes:^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "    print(bio)\n",
    "    predicted_attributes = convert_to_dict(parse_bio(bio, finetuned_bio_parser_model))\n",
    "    print(predicted_attributes)\n",
    "    print(\"predicted attributes:^^^^^^^^^^^^^^^^^\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "id": "BDSuk0AWGlOJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME\n",
      "precision: 0.8461538461538461\n",
      "recall: 0.22\n",
      "f-score: 0.5330769230769231\n",
      "---\n",
      "GENDER\n",
      "precision: 0.8809523809523809\n",
      "recall: 0.74\n",
      "f-score: 0.8104761904761905\n",
      "---\n",
      "NATIONALITY\n",
      "precision: 0.8260869565217391\n",
      "recall: 0.76\n",
      "f-score: 0.7930434782608695\n",
      "---\n",
      "BIRTH_DATE\n",
      "precision: 0.6744186046511628\n",
      "recall: 0.58\n",
      "f-score: 0.6272093023255814\n",
      "---\n",
      "BIRTH_PLACE\n",
      "precision: 0.47368421052631576\n",
      "recall: 0.36486486486486486\n",
      "f-score: 0.4192745376955903\n",
      "---\n",
      "KNOWN_FOR\n",
      "precision: 0.17857142857142858\n",
      "recall: 0.16666666666666666\n",
      "f-score: 0.17261904761904762\n",
      "---\n",
      "NOTABLE_WORKS\n",
      "precision: 0.21428571428571427\n",
      "recall: 0.2857142857142857\n",
      "f-score: 0.25\n",
      "---\n",
      "MOVEMENT\n",
      "precision: 0.3333333333333333\n",
      "recall: 0.3333333333333333\n",
      "f-score: 0.3333333333333333\n",
      "---\n",
      "ALMA_MATER\n",
      "precision: 0.2\n",
      "recall: 0.26666666666666666\n",
      "f-score: 0.23333333333333334\n",
      "---\n",
      "AWARDS\n",
      "precision: 0.3448275862068966\n",
      "recall: 0.2777777777777778\n",
      "f-score: 0.3113026819923372\n",
      "---\n",
      "ELECTED\n",
      "precision: 0.375\n",
      "recall: 0.375\n",
      "f-score: 0.375\n",
      "---\n",
      "MOTHER\n",
      "precision: 0.7391304347826086\n",
      "recall: 0.6938775510204082\n",
      "f-score: 0.7165039929015085\n",
      "---\n",
      "FATHER\n",
      "precision: 0.7441860465116279\n",
      "recall: 0.6666666666666666\n",
      "f-score: 0.7054263565891472\n",
      "---\n",
      "PARTNER\n",
      "precision: 0.7560975609756098\n",
      "recall: 0.7209302325581395\n",
      "f-score: 0.7385138967668746\n",
      "---\n",
      "CHILDREN\n",
      "precision: 0.296875\n",
      "recall: 0.2676056338028169\n",
      "f-score: 0.28224031690140844\n",
      "---\n",
      "NOTABLE_TYPE\n",
      "precision: 1.0\n",
      "recall: 0.06\n",
      "f-score: 0.53\n",
      "---\n",
      "EDUCATION\n",
      "precision: 0.5\n",
      "recall: 0.5\n",
      "f-score: 0.5\n",
      "---\n",
      "OCCUPATION\n",
      "precision: 0.06666666666666667\n",
      "recall: 0.0625\n",
      "f-score: 0.06458333333333333\n",
      "---\n",
      "DEATH_DATE\n",
      "precision: 0.5625\n",
      "recall: 0.42857142857142855\n",
      "f-score: 0.4955357142857143\n",
      "---\n",
      "DEATH_PLACE\n",
      "precision: 0.43902439024390244\n",
      "recall: 0.36\n",
      "f-score: 0.39951219512195124\n",
      "---\n",
      "DEATH_CAUSE\n",
      "precision: 0.6538461538461539\n",
      "recall: 0.4594594594594595\n",
      "f-score: 0.5566528066528067\n",
      "---\n",
      "RESTING_PLACE\n",
      "precision: 0.4827586206896552\n",
      "recall: 0.42424242424242425\n",
      "f-score: 0.4535005224660397\n",
      "---\n",
      "MAIN_INTERESTS\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "f-score: 0.0\n",
      "---\n",
      "SERVICEYEARS\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "f-score: 0.0\n",
      "---\n",
      "BIRTH_NAME\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "f-score: 0.0\n",
      "---\n",
      "ALIAS\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "f-score: 0.0\n",
      "---\n",
      "INSTRUMENT\n",
      "precision: 0.5454545454545454\n",
      "recall: 0.35294117647058826\n",
      "f-score: 0.44919786096256686\n",
      "---\n",
      "GENRE\n",
      "precision: 0.38461538461538464\n",
      "recall: 0.2777777777777778\n",
      "f-score: 0.33119658119658124\n",
      "---\n",
      "HOMETOWN\n",
      "precision: 0.42857142857142855\n",
      "recall: 0.2\n",
      "f-score: 0.3142857142857143\n",
      "---\n",
      "CITIZENSHIP\n",
      "precision: 0.16666666666666666\n",
      "recall: 0.047619047619047616\n",
      "f-score: 0.10714285714285714\n",
      "---\n",
      "YEARS_ACTIVE\n",
      "precision: 0.4\n",
      "recall: 0.21052631578947367\n",
      "f-score: 0.30526315789473685\n",
      "---\n",
      "LABEL\n",
      "precision: 0.5\n",
      "recall: 0.4166666666666667\n",
      "f-score: 0.45833333333333337\n",
      "---\n",
      "ASSOCIATED_ACTS\n",
      "precision: 0.2777777777777778\n",
      "recall: 0.19230769230769232\n",
      "f-score: 0.23504273504273504\n",
      "---\n",
      "FIELDS\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "f-score: 0.0\n",
      "---\n",
      "THESIS_TITLE\n",
      "precision: 1.0\n",
      "recall: 0.6666666666666666\n",
      "f-score: 0.8333333333333333\n",
      "---\n",
      "THESIS_YEAR\n",
      "precision: 0.6666666666666666\n",
      "recall: 0.4\n",
      "f-score: 0.5333333333333333\n",
      "---\n",
      "DOCTORAL_ADVISOR\n",
      "precision: 0.5\n",
      "recall: 0.3333333333333333\n",
      "f-score: 0.41666666666666663\n",
      "---\n",
      "INSTITUTIONS\n",
      "precision: 0.09090909090909091\n",
      "recall: 0.058823529411764705\n",
      "f-score: 0.0748663101604278\n",
      "---\n",
      "NOTABLE_STUDENTS\n",
      "precision: 1.0\n",
      "recall: 0.2\n",
      "f-score: 0.6\n",
      "---\n",
      "INFLUENCES\n",
      "precision: 0.19047619047619047\n",
      "recall: 0.19047619047619047\n",
      "f-score: 0.19047619047619047\n",
      "---\n",
      "INFLUENCED\n",
      "precision: 0.2222222222222222\n",
      "recall: 0.3333333333333333\n",
      "f-score: 0.2777777777777778\n",
      "---\n",
      "NOTABLE_ASCENTS\n",
      "precision: 0.13043478260869565\n",
      "recall: 0.11538461538461539\n",
      "f-score: 0.12290969899665552\n",
      "---\n",
      "FINAL_ASCENT\n",
      "precision: 0.5\n",
      "recall: 0.4\n",
      "f-score: 0.45\n",
      "---\n",
      "PARTNERSHIPS\n",
      "precision: 0.3333333333333333\n",
      "recall: 0.23076923076923078\n",
      "f-score: 0.28205128205128205\n",
      "---\n",
      "START_AGE\n",
      "precision: 0.375\n",
      "recall: 0.375\n",
      "f-score: 0.375\n",
      "---\n",
      "SPORT\n",
      "precision: 0.4\n",
      "recall: 0.6666666666666666\n",
      "f-score: 0.5333333333333333\n",
      "---\n",
      "COUNTRY\n",
      "precision: 0.4\n",
      "recall: 0.6666666666666666\n",
      "f-score: 0.5333333333333333\n",
      "---\n",
      "AVERAGE\n",
      "precision: 0.4276707877493838\n",
      "recall: 0.3265709766103117\n",
      "f-score: 0.37712088217984774\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "testset_filename = 'SynthBio_test.json'\n",
    "max_test_items = 50  # set to 10 for initial testing; for final testing and submission, choose a value between 50 and 237\n",
    "wiki_bio_test = load_wiki_bio_test_set(testset_filename, max_test_items)\n",
    "evaluate_on_test_set(finetuned_bio_parser_model, wiki_bio_test, threshold_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCMqucECM9EM"
   },
   "source": [
    "How well did your model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5frXHaeNFjP"
   },
   "outputs": [],
   "source": [
    "# TODO - fill in these values\n",
    "average_precision = 0.0\n",
    "average_recall = 0.0\n",
    "average_fscore = 0.0\n",
    "\n",
    "# What attributes had the highest F-scores\n",
    "best_attributes = {\n",
    "    \"attrbute_name1\" : 0.0,\n",
    "    \"attrbute_name2\" : 0.0,\n",
    "    \"attrbute_name3\" : 0.0,\n",
    "}\n",
    "\n",
    "# What attributes had the lowest F-scores\n",
    "worst_attributes = {\n",
    "    \"attrbute_name1\" : 0.0,\n",
    "    \"attrbute_name2\" : 0.0,\n",
    "    \"attrbute_name3\" : 0.0,\n",
    "}\n",
    "\n",
    "# What could you do the perform the model's performance?\n",
    "potential_improvements = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NFilM6oNv-F"
   },
   "source": [
    "# Feedback questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPS7_smBN-2n"
   },
   "outputs": [],
   "source": [
    "# How many hours did you spend on this assignment? Just an approximation is fine.\n",
    "num_hours_spent = 0\n",
    "\n",
    "# What did you think?  This was the first time we tried this assignment\n",
    "# so you're feedback is valable.\n",
    "feedback = \"\"\"\n",
    "Type your response here.\n",
    "Your response may span multiple lines.\n",
    "Do not include these instructions in your response.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
