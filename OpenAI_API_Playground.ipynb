{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjqy7heO706G"
   },
   "source": [
    "# Exploring GPT: Section detailing how to connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VpROz_FfJZn"
   },
   "source": [
    "## Using the API\n",
    "\n",
    "First, you'll need install the OpenAPI via pip.  You can use Unix commands in a colab notebook by prefixing them with an exclamation point. The example below uses a `%` prefix instead—this guarantees that the installation is done using the same Python version that your notebook is using. Change line 2 from `%pip...` to `!pip...` if you're running into trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "396iGnE4ra9g"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jdqGfOyrmhG"
   },
   "source": [
    "Next, you will enter your secret key for the OpenAI API. You can generate your OpenAI API key [here](https://platform.openai.com/api-keys). Also, start the service through the client variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PIBX87qrlDd",
    "outputId": "4d0b575f-2e1a-48f4-8734-b05e5721b66f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter OpenAI API key:\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "print('Enter OpenAI API key:')\n",
    "openai.api_key = getpass()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=openai.api_key\n",
    "\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5iKKme91RMM"
   },
   "source": [
    "# Experimentation, defining the API function calls for...\n",
    "\n",
    "### 1. City activity finder\n",
    "### 2. Meal Prep Generator\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "g5AWEh526gIw"
   },
   "outputs": [],
   "source": [
    "def city_activity(city):\n",
    "  zero_shot_prompt = \"\"\"Output a list of four reccomendations, line by line.\n",
    "                      \"\"\"\n",
    "\n",
    "  # All GPT completions are generated by the chat model now. The chat model works best\n",
    "  # when it's given a baseline instruction about how GPT should behave. We'll use this one.\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"Give a list of four activites to do in the given city, line by line.\"}\n",
    "\n",
    "  client = openai.OpenAI()\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages= [\n",
    "          system_message,\n",
    "          {\"role\" : \"user\", \"content\" : zero_shot_prompt + city + \" - \"} # simulate a user prompt\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"---\"]\n",
    "  )\n",
    "  \n",
    "  time.sleep(1)\n",
    "\n",
    "  # the response from OpenAI's API is a JSON object that contains the completion to your prompt plus some other information.  \n",
    "  # Here's how to access just the text of the completion.\n",
    "  list = response.choices[0].message.content.strip()\n",
    "  return list\n",
    "\n",
    "#######################\n",
    "#######################\n",
    "#######################\n",
    "#######################\n",
    "\n",
    "def meal_plan(allergies, fitness_goal):\n",
    "  # TODO - write this function\n",
    "  zero_shot_prompt = \"\"\"Given the dietary restrictions and fitness goals, output a list of what to eat during the week.\n",
    "                      \"\"\"\n",
    "\n",
    "  # All GPT completions are generated by the chat model now. The chat model works best\n",
    "  # when it's given a baseline instruction about how GPT should behave. We'll use this one.\n",
    "  system_message = {\"role\" : \"system\", \"content\" : \"Output a list of what to eat in the week based on the dietary restrictions and fitness goals.\"}\n",
    "\n",
    "  client = openai.OpenAI()\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages= [\n",
    "          system_message,\n",
    "          {\"role\" : \"user\", \"content\" : zero_shot_prompt + allergies + fitness_goal + \" - \"} # simulate a user prompt\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=[\"---\"]\n",
    "  )\n",
    "  \n",
    "  time.sleep(1)\n",
    "\n",
    "  # the response from OpenAI's API is a JSON object that contains\n",
    "  # the completion to your prompt plus some other information.  Here's how to access\n",
    "  # just the text of the completion.\n",
    "  summary = response.choices[0].message.content.strip()\n",
    "  return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "y8ODUSvg8dbE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Visit the Red Rocks Amphitheatre for a concert or hike among the stunning rock formations.\n",
      "2. Explore the Denver Art Museum to see a diverse collection of art from around the world.\n",
      "3. Take a stroll through the historic Larimer Square for shopping, dining, and vibrant nightlife.\n",
      "4. Head to Denver Botanic Gardens to admire beautiful plant displays and peaceful gardens.\n"
     ]
    }
   ],
   "source": [
    "city = \"\"\"\n",
    "Denver\n",
    "\"\"\"\n",
    "\n",
    "answer = city_activity(city)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "N6Y3--nI8h-b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your dietary restrictions of avoiding peanuts, strawberries, and gluten, here is a sample meal plan for the week to help you gain muscle:\n",
      "\n",
      "Monday:\n",
      "- Breakfast: Scrambled eggs with spinach and avocado\n",
      "- Snack: Greek yogurt with honey and almonds\n",
      "- Lunch: Grilled chicken with quinoa and roasted vegetables\n",
      "- Snack: Carrot sticks with hummus\n",
      "- Dinner: Baked salmon with sweet potato and asparagus\n",
      "\n",
      "Tuesday:\n",
      "- Breakfast: Oatmeal with almond butter and banana\n",
      "- Snack: Apple slices with almond butter\n",
      "- Lunch: Turkey and avocado wrap with lettuce\n",
      "- Snack: Rice cakes with tuna salad\n",
      "- Dinner: Beef stir-fry with broccoli and brown rice\n",
      "\n",
      "Wednesday:\n",
      "- Breakfast: Smoothie with spinach, banana, protein powder, and almond milk\n",
      "- Snack: Cottage cheese with pineapple\n",
      "- Lunch: Lentil soup with a side salad\n",
      "- Snack: Celery sticks with peanut butter (if tolerated)\n",
      "- Dinner: Grilled shrimp with quinoa and steamed green beans\n",
      "\n",
      "Thursday:\n",
      "- Breakfast: Greek yogurt with granola and mixed berries (excluding strawberries)\n",
      "- Snack: Rice cakes with hummus\n",
      "- Lunch: Turkey chili with a side of cornbread\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "allergies = \"\"\"\n",
    "Peanuts, strawberries, gluten.\n",
    "\"\"\"\n",
    "fitness_goals = \"\"\"\n",
    "I would like to gain 20 pounds of muscle in the next year. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "answer = meal_plan(allergies, fitness_goals)\n",
    "print(answer)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "That's it!  That's an example of how to write a function call to the OpenAI API in order for it to output a subject for a topic.\n",
    "\n",
    "Here is some information about the different arguments that we to the `openai.Completion.create` call:\n",
    " * `model` – OpenAI offers many different versions of their chat model. We'll use GPT3.5-turbo, which is functional without being [the most expensive to run](https://openai.com/api/pricing/).\n",
    " * `messages` - this is the set of messages that will instruct how the model should respond. The first message is a system prompt—an overall setting that specifies how the model should behave. In this case, the second message is the \"training set\" of examples followed by the last topic that we want to generate a subject for.\n",
    " * `temperature` - controls how much of the probability distribution the model will use when it is generating each token. 1.0 means that it samples from the complete probability distrubiton, 0.7 means that it drops the bottom 30% of the least likely tokens when it is sampling. 0.0 means that it will perform deterministically and always output the single most probable token for each context.\n",
    " * `top_p` - is an alternative way of controling the sampling.\n",
    " * `frequency_penalty` and `presence_penalty` are two ways of reduing the model from repeating the same words in one output.  You can set these to be >0 if you're seeing a lot of repetition in your output.\n",
    " * `max_tokens` is the maximum length in tokens that will be output by calling the function.  A token is a subword unit.  There are roughly 2 or 3 tokens per word on average.\n",
    " * `stop` is a list of stop sequences.  The model will stop generating output once it generates one of these strings, even if it hasn't reached the max token length. By default this is set to a special token `<|endoftext|>`.\n",
    "\n",
    "You can read more about [the Chat Completion API call in the documentation](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
